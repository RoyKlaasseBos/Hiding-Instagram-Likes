{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/instagram_header.png\" align=\"left\" style=\"margin-bottom: 20px\"/>\n",
    "\n",
    "<h2> Web Appendix - Hiding Like Counts on Instagram </h2>\n",
    "\n",
    "<p style=\"clear: both;\">This online appendix complements the master thesis \"Goodbye Likes, Hello Mental Health: How Hiding Like Counts Affects User Behavior & Self-Esteem\":</p> \n",
    "\n",
    "<p><i>Likes are widely available on social network services and are known to influence people’s self-image. An emerging literature has started to look at potential detrimental effects of social media use among teenagers. We study how Instagram users’ posting frequency, variety, like behavior, and relative self-esteem are affected by an intervention in which like counts were hidden in selected treatment countries. Using a unique panel data set of individual users’ Instagram posts across multiple years, we find evidence that users posted more frequently and more varied than in the months prior to the intervention. On the other hand, the number of likes decreases as people are no longer influenced by others’ evaluations, especially among users with a small following. Further, in an experiment we show that the number of likes people see on others’ posts affects their relative self-esteem, and that users are more likely to self-disclose once they rate themselves more positively. These results are critical to understanding the dynamics on visual-based social media in order to foster a healthy online environment.</i></p>\n",
    "\n",
    "<p>In this notebook, we perform the following steps (run this .ipynb-file locally for clickable anchors): </p>\n",
    "\n",
    "A. [Instagram Influencer Seed](#instagram-influencer-seed)  \n",
    "B. [Instagram Consumers Selection](#instagram-consumers-selection)  \n",
    "C. [Collect & Preprocess Instagram Data](#preprocess-instagram-data)  \n",
    "D. [Computer Vision](#computer-vision)  \n",
    "E. [Cosine Similarity & Image Similarity](#cosine-similarity)  \n",
    "F. [Outlier Screening](#outlier-screening)  \n",
    "G. [Propensity Score Matching](#propensity-score-matching)  \n",
    "H. [Differences in Differences](#differences-in-differences)  \n",
    "I. [Randomized Experiment](#experiment)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, pickle, random, datetime, json, requests, sys, os\n",
    "from sqlalchemy import create_engine\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "\n",
    "# define path to local SQL database\n",
    "engine = create_engine('postgresql+psycopg2://postgres:thesis@localhost:5433/thesis')\n",
    "\n",
    "# support R in Jupyter Notebook\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "set.seed(123) # for reproducibility\n",
    "library(RPostgreSQL)\n",
    "library(Matching)\n",
    "library(dplyr)\n",
    "library(plm)\n",
    "library(lme4)\n",
    "library(lmerTest)\n",
    "library(nlme)\n",
    "library(ez)\n",
    "library(ggplot2)\n",
    "library(plotrix)\n",
    "library(reshape)\n",
    "library(lsr)\n",
    "library(pscl)\n",
    "library(psych)\n",
    "library(mfx)\n",
    "library(erer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"instagram-influencer-seed\"></a>\n",
    "### A. Instagram Influencer Seed\n",
    "\n",
    "<a href=\"https://hypeauditor.com/top-instagram/\">HypeAuditor</a> lists the top influencers by  category by country. The ranking is updated daily and takes into account quality audience and authentic engagement to control for bots and inactive accounts. For each listing the number of followers from a given country and the total number of followers are indicated. First, we scraped all listings for each category and country combination in February 2020. The number of listings for each combination varies depending on the volume of influencers in the domain. In 45 cases (0.3%) the ``followers_from_country``  column was missing because the data was unavailable on HypeAuditor. These records have been excluded from our analysis. Second, we divide both metrics by one another to derive the *purity*. That is, the percentage of influencers' followers from a given country. Third, we exclude followers whose purity is below 50%. As a result, we end up with a list of influencers (N = 5391) whose main following base is located in a single country (41% of all top influencers). Fourth, we sort the influencers by purity and pick the top 20 influencers for each country in our dataset.\n",
    "\n",
    "<img src=\"./images/hypeauditor.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_follower_counts(df, column):\n",
    "    '''convert string follower counts (e.g., 3K) into numeric (e.g., 3000)'''\n",
    "    for counter in range(len(df)): \n",
    "        if \"M\" in df.loc[counter, column]:\n",
    "            df.loc[counter, column] = float(df.loc[counter, column].replace(\"M\", \"\")) * 1000000        \n",
    "        elif \"K\" in df.loc[counter, column]:\n",
    "            df.loc[counter, column] = float(df.loc[counter, column].replace(\"K\", \"\")) * 1000\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_country(df): \n",
    "    '''extract country from URL (e.g., https://hypeauditor.com/top-instagram-beer-wine-spirits-brazil/ --> brazil)'''\n",
    "    countries = [\"australia\", \"brazil\", \"canada\", \"china\", \"france\", \"germany\", \"hong-kong\", \"india\", \"indonesia\", \\\n",
    "                 \"italy\", \"malaysia\", \"mexico\", \"russia\", \"saudi-arabia\", \"slovakia\", \"spain\", \"switzerland\", \"ukraine\", \\\n",
    "                 \"united-arab-emirates\", \"united-kingdom\", \"united-states\"]\n",
    "    for counter in range(len(df)): \n",
    "        for country in countries: \n",
    "            if country in df.loc[counter, 'url']:\n",
    "                df.loc[counter, 'country'] = country\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_purity(df):\n",
    "    '''determine the percentage of influencers' followers from a given country'''\n",
    "    for counter in range(len(df)):\n",
    "        df.loc[counter, 'percentage_country'] = float(df.loc[counter, 'followers_from_country']) / float(df.loc[counter,'total_follower'])\n",
    "    return df\n",
    "\n",
    "\n",
    "# import data      \n",
    "df = pd.read_csv(\"data/hypeauditor.csv\", keep_default_na=False)\n",
    "\n",
    "# add country column to data\n",
    "df = extract_country(df)\n",
    "\n",
    "# convert follower counts to numeric\n",
    "df = convert_follower_counts(df, 'followers_from_country')\n",
    "df = convert_follower_counts(df, 'total_follower')\n",
    "\n",
    "# exclude records for which either the followers from country or total follower count is missing\n",
    "df = df[(df['followers_from_country'] != \"\") & (df['total_follower'] != \"\")].reset_index(drop=True)\n",
    "\n",
    "# add purity measure\n",
    "df = calculate_purity(df)\n",
    "\n",
    "# exclude influencers whose purity is below 50%; mean purity increases from 37.1% to 70.1%\n",
    "df = df.loc[df.percentage_country > .5]\n",
    "\n",
    "# select top influencers from each country whose purity is highest (mean purity: 82.8%)\n",
    "top_20 = df.groupby('country')['percentage_country'].nlargest(20)\n",
    "\n",
    "# obtain the usernames belonging to these influencers\n",
    "indices = [top_20.index[counter][1] for counter in range(len(top_20.index))] \n",
    "selected_df = df.loc[indices, ['username', 'country', 'percentage_country']]\n",
    "\n",
    "# export influencers' username, country of main following, and purity to local database\n",
    "# --- selected_df.to_sql(\"purity\", engine, index=None, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"instagram-consumers-selection\"> </a>\n",
    "### B. Instagram Consumers Selection\n",
    "\n",
    "For each selected influencer we collect a list of their followers using [Phantombuster](https://phantombuster.com/automations/instagram/7085/instagram-profile-scraper) (figure below), draw a random sample of  followers, and validate their country of origin in order to construct our dataset of consumers. That is, public Instagram users who i) followed an influencer account from our list of top 20 influencers by country we identified in step 1, ii) posted at least 50 pictures and/or videos (of which at least 5 before and after the intervention), iii) were followed by up to 5000 users, and iv) were not used for business purposes. We derived the latter from the Instagram account type (business or personal), whether the account was owned by an individual or an organisation, and whether posts are commercially affiliated (i.e., they promote products or services). Our sample includes personal accounts owned by individual users who do not engage in commercial activities on Instagram. \n",
    "\n",
    "<img src=\"./images/phantombuster.png\" width=\"650px\"/>\n",
    "\n",
    "\n",
    "<p style=\"clear: both;\">Furthermore, we validate the user's country of origin as follows. First, the language used in the bio and post captions should correspond with the main language in the country of origin. Second, language use in post comments should also be in line with the main language in the country of origin. Third, location tags should primarily refer to places in the country of origin (though a vacation photo taken elsewhere may sporadically occur). We repeat this process until we have gathered 40 accounts by country:</p>\n",
    "\n",
    "\n",
    "| Country | Type | #Accounts |\n",
    "| ------ | ------ |------ |\n",
    "| Australia | Treatment | 40 |\n",
    "| Canada | Treatment | 40 |\n",
    "| France | Control | 40 |\n",
    "| Germany | Control | 40 |\n",
    "| Italy | Treatment | 40 |\n",
    "| Netherlands | Control | 40 |\n",
    "| Spain | Control | 40 |\n",
    "| United Kingdom | Control | 40 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocess-instagram-data\"></a>\n",
    "### C. Collect & Preprocess Instagram Data\n",
    "In step 1 and 2 we created a list of Instagram usernames of which we collected historical post data using [Instagram Scraper](https://github.com/arc298/instagram-scraper). This is a command-line application written in Python to obtain user information, social relationship information, and photo information. Since the photo and video files attached to each post take up a significant amount of memory, we only store the links to the online media files. More specifically, we run the command below to collect the post information of usernames in `FILE_NAME.txt` (i.e., text file that contains all usernames in our sample). \n",
    "\n",
    "<img src=\"./images/instagram_scraper.png\" alt=\"Instagram Scraper Github\" align=\"left\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`instagram-scraper -f FILE_NAME.txt --media-types none --media-metadata --profile-metadata -T {username}_{urlname}`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraping process yields a separate JSON-file for each account which requires further preprocessing for follow-up analysis. For each user we extracted post and user level data and stored it into a dataframe which we then pushed to a local database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_seed(file_path):\n",
    "    '''open usernames and store in array'''\n",
    "    with open(file_path) as f:\n",
    "        names = [line.split() for line in f]\n",
    "    return [name for name_list in names for name in name_list]\n",
    "\n",
    "def pickle_files(pickle_name, df):\n",
    "    '''store output of data frame as pickle'''\n",
    "    with open(pickle_name, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "\n",
    "def parse_json_files(usernames, path):\n",
    "    '''parse the json file for each username in the text file and store preprocessed records in a data frame'''\n",
    "    \n",
    "    # declare dataframes for post data (df), user profile data (profile), and arrays for user accounts which were missing (i.e., deleted after constructing seeds) or private (i.e., user data could not be scraped)\n",
    "    df = pd.DataFrame()\n",
    "    profile = pd.DataFrame()\n",
    "    missing = []\n",
    "    private = []\n",
    "    \n",
    "    for username in usernames: \n",
    "        # for each username load raw json file in memory\n",
    "        try: \n",
    "            with open(\"./data/\" + path + \"/\" + username + \"/\" + username + \".json\") as f:\n",
    "                d = json.load(f)\n",
    "        except: \n",
    "            missing.append(username)\n",
    "\n",
    "        # test if user profile is publicly available\n",
    "        try:        \n",
    "            # post level data\n",
    "            shortcode = [d['GraphImages'][counter]['shortcode'] for counter in range(len(d['GraphImages']))]\n",
    "            description = [d['GraphImages'][counter]['edge_media_to_caption']['edges'][0]['node']['text'] if len(d['GraphImages'][counter]['edge_media_to_caption']['edges']) > 0 else \"NA\" for counter in range(len(d['GraphImages']))]\n",
    "            total_likes = [d['GraphImages'][counter]['edge_media_preview_like']['count'] for counter in range(len(d['GraphImages']))]\n",
    "            total_comments = [d['GraphImages'][counter]['edge_media_to_comment']['count'] for counter in range(len(d['GraphImages']))]\n",
    "            hashtags = [d['GraphImages'][counter]['tags'] if type(d['GraphImages'][counter].get('tags')) == list else \"NA\" for counter in range(len(d['GraphImages']))]\n",
    "            content_type = [d['GraphImages'][counter]['__typename'] for counter in range(len(d['GraphImages']))]\n",
    "            timestamp = [d['GraphImages'][counter]['taken_at_timestamp'] for counter in range(len(d['GraphImages']))]\n",
    "            video_views = [d['GraphImages'][counter]['video_view_count'] if type(d['GraphImages'][counter].get('video_view_count')) == int else 0 for counter in range(len(d['GraphImages']))]\n",
    "            media1 = [d['GraphImages'][counter]['urls'][0] for counter in range(len(d['GraphImages']))]\n",
    "\n",
    "            df_temp = pd.DataFrame({\n",
    "                           \"username\": username,\n",
    "                           \"shortcode\": shortcode, \n",
    "                           \"description\": description, \n",
    "                           \"total_likes\": total_likes,\n",
    "                           \"total_comments\": total_comments,\n",
    "                           \"hashtags\": hashtags,\n",
    "                           \"content_type\": content_type,\n",
    "                           \"timestamp\": timestamp, \n",
    "                           \"video_views\": video_views,\n",
    "                           \"media1\": media1 # for image caroussels we focused on the first photo/video\n",
    "                          })\n",
    "\n",
    "            df = pd.concat([df_temp, df]).reset_index(drop=True)\n",
    "\n",
    "            # user level profile data\n",
    "            followers_count = [d['GraphProfileInfo']['info']['followers_count']]\n",
    "            following_count = [d['GraphProfileInfo']['info']['following_count']]\n",
    "            posts_count = [d['GraphProfileInfo']['info']['posts_count']]\n",
    "            biography = [d['GraphProfileInfo']['info']['biography']]\n",
    "            full_name = [d['GraphProfileInfo']['info']['full_name']]\n",
    "\n",
    "            profile_temp = pd.DataFrame({\n",
    "                \"username\": username,\n",
    "                \"followers_count\": followers_count,  \n",
    "                \"following_count\": following_count, \n",
    "                \"posts_count\": posts_count,\n",
    "                \"biography\": biography, \n",
    "                \"full_name\": full_name \n",
    "            })\n",
    "\n",
    "            profile = pd.concat([profile_temp, profile]).reset_index(drop=True)\n",
    "\n",
    "        except: \n",
    "            private.append(username)\n",
    "\n",
    "    # convert epoch time to regular timestamp\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "\n",
    "    # add regular date (without time)\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "\n",
    "    return df, missing, private, profile\n",
    "\n",
    "# list with usernames, JSON path, user type\n",
    "paths = [[\"./data/json_consumer/consumers_new.txt\", \"json_consumer\", \"consumers\"]]\n",
    "\n",
    "# for all usernames process JSON files, push to SQL database, and store a pickle copy\n",
    "for path in paths: \n",
    "    temp_output = parse_json_files(open_seed(path[0]), path[1])\n",
    "    temp_df = temp_output[0]\n",
    "    temp_missing = temp_output[1]\n",
    "    temp_private = temp_output[2]\n",
    "    temp_profile = temp_output[3]\n",
    "    \n",
    "    temp_df.to_sql(path[2], engine, index=None, if_exists='replace')\n",
    "    temp_profile.to_sql(path[2] + \"_profile\", engine, index=None, if_exists='replace')\n",
    "    pickle_files('pickles/' + path[2] + '.pickle', temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"computer-vision\"></a>\n",
    "### D. Computer Vision\n",
    "We use Azure Cognitive Services Computer Vision Application Programming Interface ([API](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/)) to analyze image content. For every image, the API returns a vector of tags and confidence scores (figure below). First, we make an API request and pickle all output data for further analysis. Second, we compute image similarity within and between-subjects using image tags data. Note that Instagram image URLs are valid for a limited amount of time. The code sample below, therefore, only runs for recently scraped data. Expired URLs are printed in the console.\n",
    "\n",
    "<img src=\"./images/vision_api_example.png\" align=\"left\" alt=\"Computer Vision API Example (tags)\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'COMPUTER_VISION_SUBSCRIPTION_KEY' in os.environ:\n",
    "    subscription_key = os.environ['COMPUTER_VISION_SUBSCRIPTION_KEY']\n",
    "else:\n",
    "    print(\"\\nSet the COMPUTER_VISION_SUBSCRIPTION_KEY environment variable.\\n**Restart your shell or IDE for changes to take effect.**\")\n",
    "    sys.exit()\n",
    "\n",
    "if 'COMPUTER_VISION_ENDPOINT' in os.environ:\n",
    "    endpoint = os.environ['COMPUTER_VISION_ENDPOINT']\n",
    "    \n",
    "analyze_url = endpoint + \"vision/v2.1/analyze\"\n",
    "\n",
    "\n",
    "def process_image(temp_df, categories=True):\n",
    "    '''obtain categories or tags image data from all images in dataframe using Azure Cognitive Services'''\n",
    "    df = pd.DataFrame(columns=['uri', 'timestamp', 'category', 'score'])\n",
    "    \n",
    "    for counter in range(len(temp_df)):\n",
    "        image_url = temp_df.loc[counter, 'media1']\n",
    "        time_stamp = temp_df.loc[counter, 'timestamp']\n",
    "    \n",
    "        headers = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "        data = {'url': image_url}\n",
    "        params = {'visualFeatures': 'Categories'} if categories else {'visualFeatures': 'Tags'}\n",
    "        \n",
    "        try: \n",
    "            response = requests.post(analyze_url, headers=headers,\n",
    "                                 params=params, json=data)\n",
    "            output = response.json()\n",
    "\n",
    "            if categories: \n",
    "                for category in output['categories']: \n",
    "                    df = df.append(\n",
    "                        dict(\n",
    "                            uri = image_url,\n",
    "                            timestamp = time_stamp,\n",
    "                            category = category['name'],\n",
    "                            score = category['score'],\n",
    "                        ), ignore_index = True)\n",
    "\n",
    "            else: \n",
    "                for tag in output['tags']: \n",
    "                    df = df.append(\n",
    "                            dict(\n",
    "                                uri = image_url,\n",
    "                                timestamp = time_stamp,\n",
    "                                category = tag['name'],\n",
    "                                score = tag['confidence'],\n",
    "                            ), ignore_index=True)           \n",
    "                    \n",
    "        except: \n",
    "            #image url expired\n",
    "            print(image_url)             \n",
    "            \n",
    "    return df\n",
    "\n",
    "# to save time and computing resources we only collect image tags data among users selected after matching\n",
    "# in step 7 we describe the propensity score matching procedure\n",
    "connection = engine.connect()\n",
    "consumers_psm_query = connection.execute(\"SELECT * FROM consumers_psm\").fetchall()\n",
    "consumers_selected = pd.DataFrame(consumers_psm_query)[0]\n",
    "\n",
    "# consumers' post level data\n",
    "temp_df = pickle.load(open(\"pickles/consumers.pickle\", \"rb\"))\n",
    "\n",
    "for consumer in consumers_selected:\n",
    "    if not os.path.isfile('./pickles/image_output/Azure_Tags/' + consumer + '.pickle'):\n",
    "        temp = temp_df.loc[(temp_df.username == consumer) & (temp_df.content_type != \"GraphVideo\")].reset_index(drop=True) \n",
    "        image_tags = process_image(temp, False)\n",
    "        pickle_files('pickles/image_output/Azure_Tags/' + consumer + '.pickle', image_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cosine-similarity'> </a>\n",
    "### E. Cosine Similarity & Image Similarity\n",
    "\n",
    "#### E.1 Cosine Similarity\n",
    "To illustrate how the cosine similarity scores are derived, we go over a fictitious example for the within-subject design. Let's assume a user posted two pictures of which we want to compute the cosine similarity: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cosine_similarity.jpg\" align=\"left\" alt=\"Cosine Similarity Example\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As follows from the figure the computer vision algorithm API returned three tags for both pictures. The first picture contains a group of people watching the sunset together, and the second picture also shows a group of people standing in a forest. To account for uncertainty each of these tags is associated with a confidence score, which we can write down in matrix notation as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_group</th>\n",
       "      <th>outdoor</th>\n",
       "      <th>sunset</th>\n",
       "      <th>forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>picture1.jpg</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picture2.jpg</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              people_group  outdoor  sunset  forest\n",
       "picture1.jpg          0.67     0.93    0.89    0.00\n",
       "picture2.jpg          0.74     0.92    0.00    0.88"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pictures = pd.DataFrame([[0.67, 0.93, 0.89, 0.00], [0.74, 0.92, 0.00, 0.88]], columns=['people_group', 'outdoor', 'sunset', 'forest'], index=['picture1.jpg', 'picture2.jpg'])\n",
    "pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first and second row `forest` and `sunset` were assigned a confidence score of `0.00` respectively as these tags were not present in the images. Next, we perform the cosine similarity operation which measures the angle between two vectors and determines whether two vectors are pointing in the same direction. More specifically, we multiply the confidence scores of pictures 1 and 2 for each image tag (e.g., for people: 0.67 x 0.74) and divide by the multiplication of the length of both vectors. Mathematically, this can be denoted as: \n",
    "$$sim(r,c)=  (r \\cdot c)/(\\left\\Vert r \\right\\Vert \\cdot \\left\\Vert c \\right\\Vert)$$ \n",
    "<br /> \n",
    "Here $r$ and $c$ are the image vectors for picture 1 and 2 respectively, and $||r||$ is defined as $\\sqrt{r_1^2+r_2^2+ ... + r_n^2}$. A larger confidence score has a larger weight and more overlapping image tags gives a higher cosine similarity score. Filling in the confidence scores above, we find a cosine similarity of `0.63` between picture 1 and 2. The diagonal contains 1s as comparing any image with itself always yields a cosine similarity of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.63240507],\n",
       "       [0.63240507, 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we explain how we can apply cosine similarity transformations to address whether the variety of posts changes after the introduction of the intervention. First, we compute the cosine similarity between pictures taken before [after] the intervention with all other pictures taken before [after] the intervention. This gives a mean image similarity score by user: \n",
    "<br/>\n",
    "\n",
    "| username | before_after | image_similarity |\n",
    "| -------  | -------- | --------- | \n",
    "| alexanderkuckart | before | 0.2140 | \n",
    "| alexanderkuckart | after | 0.1984 | \n",
    "| ... | ... | ... | \n",
    "| xannabellex27 | before | 0.3134 | \n",
    "| xannabellex27 | after | 0.3087 | \n",
    "\n",
    "Again, let's consider a hypothethical user who used to only share like-seeking selfies on Instagram. After hiding like counts about half of the posts still include selfies, but the remaining posts include other subjects (e.g. scenery). This implies that the image similarity would drop since the cosine similarity of a blend of selfies and scenery photos is lower than the cosine similarity among a homogeneous sample of selfies. \n",
    "\n",
    "#### E.2 Image Similarity\n",
    "\n",
    "First, we make within-subject comparisons to address whether the variety of posts changes after the introduction of the intervention. Second, we make between-subjects comparisons to determine whether treated users share more unique content relative to others. \n",
    "\n",
    "*Within-subject similarity*  \n",
    "For each user i we compute the cosine similarity between pictures taken by the same user $i$. We distinguish between pictures taken before ($1_{before}$…$n_{before}$) and after  \n",
    "($1_{after}$…$n_{after}$) the intervention. This yields a similarity matrix in which each picture *before* [*after*] the intervention is compared with all pictures *before* [*after*] hiding like counts (i.e., white squares in figure below). Each row ($r$) and column ($c$) name present a picture from user $i$ in $k$, where $k$ can take on the value *before* or *after*. Given these two separate subsets $k$, we calculate how similar each picture on average is to all other pictures in the same subset. That is, for each row we take the row average excluding the diagonal values. Finally, we aggregate the results across all rows in $k$:\n",
    "\n",
    "\n",
    "$\\omega_{ik} = \\frac{1}{n_{ik}(n_{ik}-1)}\\sum_{r=1}^{n} \\sum_{c=1|c \\neq r}^{i-1} sim(r_{ik},c_{ik})$ \n",
    "\n",
    "The row and column names represent pictures before and after the intervention for user *i* ($\\mu_i$). Values in the matrix denote the cosine similarity for each picture pair (only the diagonal of 1s have been reported).  For the purpose of this analysis we restrict ourselves to the white squares in the top left and bottom right quadrant of the figure. Within these areas we compute row means (excluding the diagonal values) to determine how similar a given picture is to all other pictures in the same subset on average. Thereafter, we derive the before and after within-subject similarity by taking the average of the row means in the top left and bottom right squares, respectively. Note: calculating column means, rather than row means, yields identical outcomes. \n",
    "\n",
    "<img src=\"./images/within_subjects_similarity.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def within_subject_similarity(consumers, consumers_selected, categories=False):\n",
    "    '''compute the within-subject similarity from image tags before and after the intervention'''\n",
    "\n",
    "    for consumer in consumers_selected:\n",
    "        if categories: \n",
    "            image_data = pickle.load(open(\"pickles/image_output/Azure_Categories/\" + consumer + \".pickle\", \"rb\"))\n",
    "        else: \n",
    "            image_data = pickle.load(open(\"pickles/image_output/Azure_Tags/\" + consumer + \".pickle\", \"rb\"))\n",
    "            \n",
    "        consumers_df = pd.merge(image_data, consumers, left_on='uri', right_on='media1')[['uri', 'category', 'score', 'before_after']]\n",
    "\n",
    "        # turn image categories/tags into a matrix (rows: images, columns: categories/tags) and order by date (year & month)\n",
    "        tags_matrix = consumers_df.pivot_table(index=[\"before_after\", \"uri\"], columns=\"category\")\n",
    "        tags_matrix = tags_matrix.fillna(0)\n",
    "        similarity = cosine_similarity(tags_matrix)\n",
    "\n",
    "        try: \n",
    "            before_intervention = len(tags_matrix.loc[('before')])\n",
    "            after_intervention = len(tags_matrix.loc[('after')])\n",
    "\n",
    "            before_similarity = pd.Series([similarity[counter][list(range(0, counter)) + list(range(counter + 1, before_intervention))].mean() for counter in range(before_intervention)]).mean()\n",
    "            similarity_scores.loc[len(similarity_scores) + 1,] = [consumer, 'before', before_similarity]\n",
    "\n",
    "            after_similarity = pd.Series([similarity[counter][list(range(before_intervention, counter)) + list(range(counter + 1, before_intervention + after_intervention))].mean() for counter in range(before_intervention, before_intervention + after_intervention)]).mean()\n",
    "            similarity_scores.loc[len(similarity_scores) + 1,] = [consumer, 'after', after_similarity]\n",
    "        \n",
    "        except: \n",
    "            pass\n",
    "            \n",
    "    return similarity_scores \n",
    "\n",
    "# load consumers data and extract year and month from dates            \n",
    "connection = engine.connect()\n",
    "consumers_before_after_query = \"SELECT c.username, media1, CASE WHEN cc.country = 'canada' AND c.date > '2019-04-30' THEN 'after' WHEN c.date > '2019-07-17' THEN 'after' ELSE 'before' END as before_after FROM consumers c INNER JOIN consumers_psm cp ON c.username = cp.username INNER JOIN consumers_country cc ON c.username = cc.username WHERE c.date > '2018-04-30' AND c.date < '2020-04-30'\"\n",
    "consumers_before_after = connection.execute(consumers_before_after_query).fetchall()\n",
    "consumers_before_after_df = pd.DataFrame(consumers_before_after).rename({0: 'username', 1: 'media1', 2: 'before_after'}, axis=1)\n",
    "\n",
    "# compute within-subject similarity \n",
    "similarity_scores_tags = within_subject_similarity(consumers_before_after_df, consumers_before_after_df['username'].unique())\n",
    "similarity_scores_tags.to_sql(\"image_similarity_within_tags\", engine, index=None, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Between-subjects similarity*  \n",
    "To assess between-subjects similarity (B) we distinguish between cohorts of users in the treatment and control group. We choose for these comparisons for two reasons. First, Instagram users may especially stay on top of the trends in their local market and therefore their postings might have already been more like other treatment units prior to the intervention. Second, by defining cohorts we establish more homogeneous clusters of users. Within these two cohorts, we determine the cosine similarity of each user pair ($u_i, u_j$) in k = {before, after} (i.e., white squares in figure below). That is, how similar pictures from user i are to pictures from another user j on average, where $u_i$  and $u_j$  belong to the same cohort.\n",
    "\n",
    "$B_{ijk} = \\frac{1}{n_i n_j} \\sum_{r=1}^{n} \\sum_{c=1}^{n} sim(r_{ijk},c_{ijk})$\n",
    "\n",
    "The row and column names represent pictures before the intervention for user i ($u_i$) and user j ($u_j$) in the same cohort. Values in the matrix denote the cosine similarity for each picture pair (note: values are left out for simplicity). For the purpose of this analysis we restrict ourselves to the top right or the bottom left white square. Within this area we compute row means to determine how similar a given picture from $u_i$ [$u_j$] is to all pictures from $u_j$ [$u_i$] on average. Thereafter, we sum up the row means and divide by the number of rows ($n_{ik}$ [$n_{jk}$]) to derive the before between-subjects similarity. In a similar fashion, the after between-subjects similarity can be determined. Note that only one of both white squares should be used to avoid duplicates.\n",
    "\n",
    "<img src=\"./images/between_subjects_similarity.png\" width=\"500px\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_image_output(consumer, consumers, before_after='before'):\n",
    "    '''construct cosine similarity matrix for either all images before or after the intervention'''\n",
    "    image_input = pickle.load(open(\"pickles/image_output/Azure_Tags/\" + consumer + \".pickle\", \"rb\"))\n",
    "    consumers_df = pd.merge(image_input, consumers, left_on='uri', right_on='media1')[['uri', 'category', 'score', 'before_after', 'treatment_control']]\n",
    "    tags_matrix = consumers_df.pivot_table(index=[\"before_after\", \"uri\"], columns=\"category\")\n",
    "    tags_matrix = tags_matrix.fillna(0)\n",
    "    if before_after == 'before':\n",
    "        tags_matrix = tags_matrix.loc[('before')]\n",
    "    else: \n",
    "        tags_matrix = tags_matrix.loc[('after')]    \n",
    "    return tags_matrix\n",
    "\n",
    "\n",
    "def between_subjects_similarity(consumers):\n",
    "    '''compute the between-subjects similarity from image tags before and after the intervention'''\n",
    "    for consumer1 in consumers.username.unique():\n",
    "        for consumer2 in consumers.username.unique(): \n",
    "            b_similarity_scores = pd.DataFrame(columns = ['username1' , 'username2', 'username1_2', 'before_after', 'similarity']) \n",
    "           \n",
    "            try: \n",
    "                if consumer1 != consumer2: # do not compare image data of the same user\n",
    "                    for before_after in ['before', 'after']: # run this procedure for images before and after the intervention separately\n",
    "                        consumer1_df = create_image_output(consumer1, consumers, before_after)\n",
    "                        consumer2_df = create_image_output(consumer2, consumers, before_after)\n",
    "                        consumers_1_2 = pd.concat([consumer1_df, consumer2_df])\n",
    "                        consumers_1_2 = consumers_1_2.fillna(0)\n",
    "                        similarity = cosine_similarity(consumers_1_2)\n",
    "\n",
    "                        # for each image of consumer 1 take the mean cosine similarity with all image of consumer 2\n",
    "                        comparisons = [similarity[counter][len(consumer1_df):].mean() for counter in range(len(consumer1_df))]\n",
    "\n",
    "                        # aggregate the results across all images of consumer 1 (so take the mean of all mean cosine similarities) \n",
    "                        before_after_similarity = pd.Series(comparisons).mean() \n",
    "                        b_similarity_scores = b_similarity_scores.append({\n",
    "                                                                \"username1\": consumer1,\n",
    "                                                                \"username2\": consumer2,\n",
    "                                                                \"username1_2\": \"_\".join(sorted([consumer1, consumer2])),\n",
    "                                                                \"before_after\": before_after, \n",
    "                                                                \"similarity\": before_after_similarity\n",
    "                                                                }, ignore_index=True)\n",
    "\n",
    "                b_similarity_scores.to_sql(\"image_similarity_between_tags\", engine, index=None, if_exists='append')\n",
    "\n",
    "            except: \n",
    "                pass        \n",
    "\n",
    "# load consumers data and extract year and month from dates            \n",
    "connection = engine.connect()\n",
    "consumers_before_after_treatment_control_query = \"SELECT c.username, media1, CASE WHEN cc.country = 'canada' AND c.date > '2019-04-30' THEN 'after' WHEN c.date > '2019-07-17' THEN 'after' ELSE 'before' END as before_after, CASE WHEN cc.country IN ('australia', 'canada', 'italy') THEN 'treatment' ELSE 'control' END as treatment_control FROM consumers c INNER JOIN consumers_psm cp ON c.username = cp.username INNER JOIN consumers_country cc ON c.username = cc.username WHERE c.date > '2018-04-30' AND c.date < '2020-04-30'\"\n",
    "consumers_before_after_treatment_control = connection.execute(consumers_before_after_treatment_control_query).fetchall()\n",
    "consumers = pd.DataFrame(consumers_before_after_treatment_control).rename({0: 'username', 1: 'media1', 2: 'before_after', 3: 'treatment_control'}, axis=1)\n",
    "\n",
    "# compute between-subjects similarity \n",
    "between_subjects_similarity(consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='outlier-screening'></a>\n",
    "### F. Outlier Screening\n",
    "Even though we use a stringent [Instagram consumers selection](#instagram-consumers-selection) procedure, it may sporadically occur that a user systematically differs in terms of the number of followers, followings, and the average number of likes of their image posts. To overcome this issue we use a multivariate outlier screening approach and remove these users from our sample before propensity score matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# connect to local database\n",
    "drv = dbDriver(\"PostgreSQL\")\n",
    "con = dbConnect(drv, host='localhost', port='5433', dbname='thesis',\n",
    "                user='postgres', password='admin')\n",
    "\n",
    "# split query into subqueries\n",
    "query_1 = \"SELECT x.username, followers_count, following_count, average_likes FROM\" \n",
    "query_2 = \"x INNER JOIN (SELECT username, AVG(total_likes) as average_likes FROM\"\n",
    "query_3 = \"l GROUP BY username) l ON x.username = l.username\"\n",
    "\n",
    "query_user_data = function(user_profile, user){\n",
    "    # paste queries and determine follower count, following count, and average number of likes by user\n",
    "    query = paste(query_1, user_profile, query_2, user, query_3)\n",
    "    return(dbGetQuery(con, query))\n",
    "}\n",
    "\n",
    "outlier_screening = function(df){\n",
    "    # determine if the mahalanobis distance exceeds the threshold value\n",
    "    mahal = mahalanobis(df[,-1], colMeans(df[,-1]), cov(df[,-1]), tol=1e-20)\n",
    "    cutoff = qchisq(1-0.001, ncol(df[,-1]))\n",
    "    outliers = subset(df, mahal > cutoff)    \n",
    "    no_outliers = subset(df, mahal < cutoff)\n",
    "    \n",
    "    return(c(outliers, no_outliers))\n",
    "}\n",
    "\n",
    "\n",
    "remove_record = function(table, usernames){\n",
    "    # remove all posts of users labeled as outliers\n",
    "    for(username in usernames){\n",
    "        del_query = paste(\"DELETE FROM \", table, \" WHERE username='\", username, \"'\", sep=\"\")\n",
    "        dbGetQuery(con, del_query)\n",
    "    }\n",
    "}\n",
    "\n",
    "# collect user stats and then screen for outliers\n",
    "consumers_stats = query_user_data(\"consumers_profile\", \"consumers\")\n",
    "consumers_screening = outlier_screening(consumers_stats)    \n",
    "\n",
    "# remove outliers from analysis (don't run this cell twice to avoid removing outliers after already excluding outliers)\n",
    "remove_record(\"consumers\", consumers_screening[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='propensity-score-matching'></a>\n",
    "### G. Propensity Score Matching\n",
    "To reduce bias of distribution overlap and different density weighting, we rebalance our data through matching non-treated users to treated ones on similar covariate values. First, we estimate a probit model of receiving treatment on the number of followers, the number of followings, the adoption speed of Instagram users, and the percentage of image posts relative to all types of media posts. Second, we compute the Mahalanobis distance for each treated and control user pair and select unique matches sequentially, in order of closeness of their Mahalanobis distances. We match without replacement such that control units are only allowed to be used as a match once. Each treatment unit is matched with a single control unit as a higher number of matches deteriorates matching quality significantly. Third, we conduct an imbalance check before and after matching of which the results are reported in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# prepare data for propensity score matching (treatment/control country), follower/following count, image share, days since adoption\n",
    "consumers_query = \n",
    "\"\n",
    "SELECT w.username, treatment_control, followers_count, following_count, image, days_since_adoption \n",
    "FROM consumers_profile w \n",
    "INNER JOIN \n",
    "    (SELECT username, CASE     \n",
    "        WHEN country in ('australia', 'brazil', 'canada', 'italy') THEN 1   \n",
    "        ELSE 0 END as treatment_control \n",
    "    FROM consumers_country cc WHERE country != 'brazil') x ON w.username = x.username\n",
    "\n",
    "INNER JOIN (SELECT w.username, AVG(CAST(image_count AS DECIMAL) / CAST(posts_count AS DECIMAL)) as image \n",
    "            FROM consumers_profile w \n",
    "            INNER JOIN (SELECT username, SUM(CASE WHEN content_type = 'GraphImage' THEN 1 END) as image_count \n",
    "            FROM consumers GROUP BY username) l ON l.username = w.username GROUP BY w.username) y ON y.username = w.username \n",
    "\n",
    "INNER JOIN (SELECT username, DATE_PART('day', '2020-06-01'::timestamp - MIN(timestamp)) as days_since_adoption \n",
    "            FROM consumers GROUP BY username) z ON z.username = y.username\n",
    "\"\n",
    "\n",
    "PSM = function(df){  \n",
    "    # propensity score matching\n",
    "    Tr = cbind(as.vector(df$treatment_control))\n",
    "    X = as.matrix(df[,c('followers_count', 'following_count', 'days_since_adoption', 'image')])\n",
    "    \n",
    "    # replace NAs in the image column with zero\n",
    "    X[is.na(X)] = 0 \n",
    "    \n",
    "    glm1 = glm(Tr ~ X, family=binomial)\n",
    "    \n",
    "    rr1 = Match(Tr=Tr, X=glm1$fitted, replace = FALSE, Weight=1, M=1)\n",
    "    summary(rr1)\n",
    "  \n",
    "    # check balancing properties (results may deviate between bootstrap iterations)\n",
    "    MatchBalance(Tr ~ X, match.out = rr1, nboots=10000)\n",
    "  \n",
    "    # store indices of matched users\n",
    "    treatment = data.frame(df[rr1$index.treated,'username'], 'treatment')\n",
    "    colnames(treatment) = c(\"username\", \"type\")\n",
    "    control = data.frame(df[rr1$index.control,'username'], 'control')\n",
    "    colnames(control) = c(\"username\", \"type\")\n",
    "    return(rbind(treatment, control))\n",
    "}\n",
    "\n",
    "consumers_PSM_input = dbGetQuery(con, consumers_query)\n",
    "\n",
    "# lines below are commented to ensure consistency with paper (PSM results may slightly deviate for each run)\n",
    "# consumers_PSM = PSM(consumers_PSM_input)\n",
    "# dbWriteTable(con, \"consumers_psm\", consumers_PSM, overwrite = TRUE, row.names = FALSE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='differences-in-differences'></a>\n",
    "### H. Difference in Differences\n",
    "In line with our hypotheses, we examine posting frequency (H1), variety (H2), and like behavior (H3). To this end, we query the local data base and apply a difference in differences (DiD) approach to estimate the effect of hiding like counts on our matched sample of users. \n",
    "\n",
    "We compare the outcome measures of Instagram users in the treatment countries with those in control countries. As Canadians enter the treatment group prior to Australians, Brazilians, and Italians, we estimate a DiD where the time variable is relative to the intervention date.\n",
    "\n",
    "$Y_{it} = \\alpha_i + \\gamma_t + I_{t} + \\tau_{i} \\cdot I_{t}  + \\epsilon_{it}$\n",
    "\n",
    "where $Y_it$ is the dependent variable for user i at time t, \n",
    "$\\alpha_i$ is a user-level fixed effect, \n",
    "$\\gamma_t$ is a trend variable, \n",
    "$\\tau_{i}$ is 1 of if user i was assigned to the treatment group and 0 otherwise, \n",
    "$I_{t}$ is 1 if the intervention was implemented at time t and 0 otherwise, \n",
    "$\\epsilon_{it}$ is the error term for user i at time t \n",
    "\n",
    "User level fixed effects control for time-invariant user characteristics. Intervention 1 and 2 take place in late April and mid-July, respectively. Given above equation, we are especially interested in the coefficient estimate and significance of the interaction between the treatment group and intervention variables as this indicates whether treatment units respond significantly different to the intervention than control units. To account for any serial correlation, we use robust standard errors clustered at the user level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H.1 Posting frequency\n",
    "We run a difference in differences model on the monthly number of Instagram posts and interpret the coefficients. Reported R-Square values are obtained by running a linear model with user fixed effects. The model coefficients relate to the regression output as follows:\n",
    "\n",
    "| Coefficient | Regression Output | \n",
    "| :--- | :--- |\n",
    "| $\\gamma_t$ | `counter` |\n",
    "| $I_{t}$ | `interventionTRUE` |\n",
    "| $\\tau_{i} \\cdot I_{t}$ | `treatment:interventionTRUE`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# for each user collect the number of posts, mean number of likes per post, and mean number of comments per post in each month\n",
    "posts_likes_comments_query = \n",
    "\"\n",
    "SELECT c.username, followers_count, following_count, treatment, \n",
    "CASE WHEN country = 'canada' THEN date_part('year', age(month, '2019-05-01')) * 12 + date_part('month', age(month, '2019-05-01'))\n",
    "WHEN country != 'canada' THEN date_part('year', age(month, '2019-08-01')) * 12 + date_part('month', age(month, '2019-08-01')) END as months_since_intervention,\n",
    "posts, likes, comments\n",
    "FROM\n",
    "(SELECT c.username, country, followers_count, following_count,\n",
    " to_date(concat_ws('-', date_part('year', timestamp), date_part('month', timestamp), '1'), 'YYYY-MM-DD') as month, \n",
    " COUNT(DISTINCT(shortcode)) as posts, \n",
    " CASE WHEN country IN ('australia', 'canada', 'italy') THEN 1 ELSE 0 END as treatment,\n",
    " AVG(total_likes) as likes,\n",
    "AVG(total_comments) as comments\n",
    "FROM consumers c\n",
    "INNER JOIN consumers_country cc ON cc.username = c.username\n",
    "INNER JOIN consumers_profile cp ON c.username = cp.username \n",
    "INNER JOIN consumers_psm cpsm ON cpsm.username = c.username\n",
    "WHERE CASE WHEN country = 'canada' THEN DATE(timestamp) >= '2018-04-30' AND DATE(timestamp) <= '2020-04-30'\n",
    "WHEN country != 'canada' THEN DATE(timestamp) >= '2018-07-17' AND DATE(timestamp) <= '2020-07-17' END\n",
    "GROUP BY c.username, followers_count, following_count, treatment, cc.country, to_date(concat_ws('-', date_part('year', timestamp), date_part('month', timestamp), '1'), 'YYYY-MM-DD')) as c\n",
    "\"\n",
    "\n",
    "posts = dbGetQuery(con, posts_likes_comments_query)\n",
    "posts$after = posts$months_since_intervention >= 0 # create boolean that indicates whether the intervention was in place in a given month\n",
    "posts$counter = posts$months_since_intervention # copy variable for the panel data analysis (see below)\n",
    "    \n",
    "fill_missing_months = function(df){\n",
    "    # if users do not post in a given month we do not have any record of this. From this we can deduce that the number of posts in that month equals zero. This function searches for missing months and adds these records to the data frame.\n",
    "    for(username in unique(df$username)){\n",
    "        df_user = df[df$username == username,] \n",
    "        min_counter = min(df_user[, 'counter'])\n",
    "        max_counter = max(df_user[, 'counter'])\n",
    "        \n",
    "        for(counter in min_counter:max_counter){\n",
    "            if(!counter %in% df_user$counter){\n",
    "                df[nrow(df) + 1, ] = c(username, df_user[1,'followers_count'], df_user[1,'following_count'], df_user[1,'treatment'], counter, 0, 0, 0, df_user[1,'after'], counter)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    num_columns = c('followers_count', 'following_count', 'treatment', 'months_since_intervention', 'posts', 'likes', 'comments', 'counter')\n",
    "    df[, num_columns] = sapply(df[, num_columns], as.numeric) \n",
    "    return(df)\n",
    "}\n",
    "\n",
    "posts = fill_missing_months(posts)\n",
    "\n",
    "# log-transform posting frequency to account for skewness\n",
    "posts[posts$posts == 0, 'posts'] = 0.0001\n",
    "posts$log_posts = log(posts$posts)\n",
    "posts$after = as.logical(posts$after)\n",
    "posts$before = 1-posts$after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oneway (individual) effect Within Model\n",
      "\n",
      "Call:\n",
      "plm(formula = as.formula(paste(\"log_posts\", \"~ treatment + after + treatment:after + counter + counter:treatment + after:treatment:counter\")), \n",
      "    data = df.p, model = \"within\")\n",
      "\n",
      "Unbalanced Panel: n = 238, T = 1-25, N = 5016\n",
      "\n",
      "Residuals:\n",
      "       Min.     1st Qu.      Median     3rd Qu.        Max. \n",
      "-9.93207332 -1.00494633  0.00010968  1.39768620  8.21545595 \n",
      "\n",
      "Coefficients:\n",
      "                             Estimate Std. Error  t-value  Pr(>|t|)    \n",
      "afterTRUE                    4.649928   0.149663  31.0693 < 2.2e-16 ***\n",
      "counter                     -0.304943   0.010933 -27.8912 < 2.2e-16 ***\n",
      "treatment:afterTRUE         -0.804188   0.222859  -3.6085 0.0003111 ***\n",
      "treatment:counter           -0.147786   0.016576  -8.9157 < 2.2e-16 ***\n",
      "treatment:afterTRUE:counter  0.465593   0.025173  18.4956 < 2.2e-16 ***\n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
      "\n",
      "Total Sum of Squares:    39812\n",
      "Residual Sum of Squares: 25925\n",
      "R-Squared:      0.34883\n",
      "Adj. R-Squared: 0.31582\n",
      "F-statistic: 511.381 on 5 and 4773 DF, p-value: < 2.22e-16\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# run a Hausman test comparing random and fixed effects \n",
    "df.p = pdata.frame(posts, index=c('username', 'months_since_intervention'))\n",
    "fixed_effects_posts = plm(as.formula(paste('log_posts', '~ treatment + after + treatment:after + counter + counter:treatment + after:treatment:counter')), data=df.p, model='within')\n",
    "random_effects_posts = plm(as.formula(paste('log_posts', '~ treatment + after + treatment:after + counter + counter:treatment + after:treatment:counter')), data=df.p, model='random')\n",
    "phtest(fixed_effects_posts, random_effects_posts) # choose for fixed effects (p < .001)\n",
    "summary(fixed_effects_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H.2 Variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Warning:\n",
      "R[write to console]:  Converting \"username\" to factor for ANOVA.\n",
      "\n",
      "R[write to console]: Warning:\n",
      "R[write to console]:  Data is unbalanced (unequal N per group). Make sure you specified a well-considered value for the type argument to ezANOVA().\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ANOVA\n",
      "           Effect DFn DFd         F         p p<.05          ges\n",
      "2       treatment   1 227 0.1333997 0.7152729       0.0005183815\n",
      "3           after   1 227 0.4248760 0.5151734       0.0002197592\n",
      "4 treatment:after   1 227 0.3231539 0.5702803       0.0001671541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# collect data for within-subject image similarity\n",
    "within_subject_tags_query = \n",
    "\"\n",
    "SELECT ist.username, CASE WHEN cc.country IN ('australia', 'canada', 'italy') THEN 1 ELSE 0 END as treatment, \n",
    "CASE WHEN before_after = 'before' THEN 0 ELSE 1 END as after, image_similarity \n",
    "FROM image_similarity_within_tags ist \n",
    "INNER JOIN consumers_psm cp ON cp.username = ist.username \n",
    "INNER JOIN consumers_country cc ON cc.username = ist.username;\n",
    "\"\n",
    "within_subjects_tags = dbGetQuery(con, within_subject_tags_query)\n",
    "within_subjects_tags = within_subjects_tags[!duplicated(within_subjects_tags),] \n",
    "within_subjects_tags$after = factor(within_subjects_tags$after)\n",
    "within_subjects_tags$treatment = factor(within_subjects_tags$treatment)\n",
    "\n",
    "# image similarity did not differ between both treatment conditions nor before and after the intervention\n",
    "ezANOVA(data = within_subjects_tags, \n",
    "        wid = username, \n",
    "        within = .(after), \n",
    "        between = .(treatment),\n",
    "        dv = image_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Warning:\n",
      "R[write to console]:  Converting \"username1_2\" to factor for ANOVA.\n",
      "\n",
      "R[write to console]: Warning:\n",
      "R[write to console]:  Data is unbalanced (unequal N per group). Make sure you specified a well-considered value for the type argument to ezANOVA().\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ANOVA\n",
      "           Effect DFn   DFd         F            p p<.05          ges\n",
      "2       treatment   1 13810 71.801250 2.618727e-17     * 4.617082e-03\n",
      "3           after   1 13810  8.931177 2.808368e-03     * 6.974213e-05\n",
      "4 treatment:after   1 13810 69.793206 7.208961e-17     * 5.447451e-04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# collect data for between-subjects image similarity (only compare treatment units with treatment units or control units with control units)\n",
    "between_subjects_tags_query = \n",
    "\"\n",
    "SELECT username1, username2, username1_2, \n",
    "CASE WHEN before_after = 'after' THEN 1 ELSE 0 END as after,\n",
    "CASE WHEN cc1.country IN ('australia', 'canada', 'italy') THEN 1 ELSE 0 END as treatment,\n",
    "CAST(similarity as numeric)\n",
    "FROM image_similarity_between_tags i\n",
    "INNER JOIN consumers_country cc1 ON i.username1 = cc1.username\n",
    "INNER JOIN consumers_country cc2 ON i.username2 = cc2.username\n",
    "WHERE (cc1.country IN ('australia', 'canada', 'italy') AND cc2.country IN ('australia', 'canada', 'italy'))\n",
    "OR (cc1.country NOT IN ('australia', 'canada', 'italy') AND cc2.country NOT IN ('australia', 'canada', 'italy'))\n",
    "\"\n",
    "between_subjects_tags = dbGetQuery(con, between_subjects_tags_query)\n",
    "\n",
    "# remove duplicates\n",
    "between_subjects_tags = between_subjects_tags[!duplicated(between_subjects_tags[,c('username1_2', 'after')]),]\n",
    "between_subjects_tags$after = factor(between_subjects_tags$after)\n",
    "between_subjects_tags$treatment = factor(between_subjects_tags$treatment)\n",
    "\n",
    "# the treatment group reacted significantly different to the intervention than the control group (repeated-measures ANOVA)\n",
    "ezANOVA(data = between_subjects_tags, \n",
    "        wid = username1_2, \n",
    "        within = .(after), \n",
    "        between = .(treatment),\n",
    "        dv = similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H.3 Like Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oneway (individual) effect Random Effect Model \n",
      "   (Swamy-Arora's transformation)\n",
      "\n",
      "Call:\n",
      "plm(formula = as.formula(paste(\"log_likes1000\", \"~ treatment + after + treatment:after + counter + followers_count + following_count\")), \n",
      "    data = df.p, model = \"random\")\n",
      "\n",
      "Unbalanced Panel: n = 238, T = 1-25, N = 4436\n",
      "\n",
      "Effects:\n",
      "                   var  std.dev share\n",
      "idiosyncratic 111290.1    333.6 0.182\n",
      "individual    498599.0    706.1 0.818\n",
      "theta:\n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      " 0.5728  0.8862  0.8974  0.8926  0.9040  0.9059 \n",
      "\n",
      "Residuals:\n",
      "    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n",
      "-2149.22  -180.64    19.13     0.36   190.44  1640.88 \n",
      "\n",
      "Coefficients:\n",
      "                       Estimate  Std. Error z-value  Pr(>|z|)    \n",
      "(Intercept)         3280.592227   93.083612 35.2435 < 2.2e-16 ***\n",
      "treatment            -94.735094   92.765368 -1.0212  0.307144    \n",
      "afterTRUE            -66.739973   22.063356 -3.0249  0.002487 ** \n",
      "counter                6.150829    1.444412  4.2584 2.059e-05 ***\n",
      "followers_count        0.854000    0.062661 13.6289 < 2.2e-16 ***\n",
      "following_count       -0.081296    0.058534 -1.3889  0.164874    \n",
      "treatment:afterTRUE  -44.965440   20.356808 -2.2089  0.027184 *  \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
      "\n",
      "Total Sum of Squares:    527110000\n",
      "Residual Sum of Squares: 493030000\n",
      "R-Squared:      0.06468\n",
      "Adj. R-Squared: 0.063413\n",
      "Chisq: 306.152 on 6 DF, p-value: < 2.22e-16\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "likes = dbGetQuery(con, posts_likes_comments_query)\n",
    "likes$after = likes$months_since_intervention >= 0 \n",
    "likes$counter = likes$months_since_intervention\n",
    "\n",
    "for(counter in 1:nrow(likes)){\n",
    "    if(likes[counter, 'likes'] == 0){\n",
    "        likes[counter, 'log_likes'] = 0 # to avoid log-transformation issues\n",
    "    } else {\n",
    "        likes[counter, 'log_likes'] = log(likes[counter, 'likes'])  \n",
    "    }         \n",
    "}\n",
    "\n",
    "df.p = pdata.frame(likes, index=c('username', 'months_since_intervention'))\n",
    "df.p$log_likes1000 = df.p$log_likes * 1000 # so that coefficients can be easier interpreted\n",
    "fixed_effects_likes = plm(as.formula(paste('log_likes1000', \"~ treatment + after + treatment:after + counter + followers_count + following_count\")), data=df.p, model='within')\n",
    "random_effects_likes = plm(as.formula(paste('log_likes1000', \"~ treatment + after + treatment:after + counter + followers_count + following_count\")), data=df.p, model='random')\n",
    "phtest(fixed_effects_likes, random_effects_likes) # choose for random effects (p > .05)\n",
    "\n",
    "# number of likes goes down after the intervention, especially among small-scale audiences\n",
    "summary(random_effects_likes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### I. Randomized Experiment\n",
    "<a id=\"experiment\"></a>\n",
    "\n",
    "<h5> I.1 Manipulation</h5>\n",
    "<p>Our responses were recruited via a questionnaire-based <a href=\"https://tilburgss.co1.qualtrics.com/jfe/form/SV_brsLIHF0unNdsBT\">experiment</a> on Prolific. We required workers to be aged between 18 to 30 years and active Instagram users to ensure they are familiar with the platform dynamics. Participants were randomly assigned to one of three same-gender conditions: high likes (128), low likes (15), and hidden likes. Individuals who self-identify as \"Other\" in the gender question were shown a picture of a woman.</p>\n",
    "\n",
    "<img src=\"./images/experiment_pictures.png\" align=\"left\" width=\"800px\"/>\n",
    "\n",
    "<h5 style=\"clear: both;\"> I.2 Target- and self-ratings</h5>\n",
    "<p>They were asked to imagine that the person portrayed on the photo was their neighbor. Then, we asked participants to make specific evaluations of themselves and the target person in terms of likeability, popularity, and attractiveness:</p>\n",
    "\n",
    "<img src=\"./images/target_self_ratings.png\" alt=\"Questionnaire self and target ratings\" align=\"left\" width=\"600px\"/>\n",
    "\n",
    "<h5 style=\"clear: both;\"> I.3 User behavior & demographics</h5>\n",
    "<p>Thereafter, we measured user’s intention to like, comment, or share the picture after viewing the post. Finally, the questionnaire concluded by collecting the frequency of Instagram use and demographic information of the participants such as age and ethnicity. We received 600 responses which we analyze in the code blocks below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# import data\n",
    "df = read.csv('./data/experiment.csv')\n",
    "\n",
    "# exclude unknown genders (users who filled out \"Other\")\n",
    "df = df[df$gender %in% c(1,2),]\n",
    "\n",
    "# convert to factor\n",
    "df$ethnicity = as.factor(df$ethnicity)\n",
    "\n",
    "# construct reliability (Cronbach-Alpha)\n",
    "alpha(df[,c('other.evaluation_1', 'other.evaluation_2', 'other.evaluation_3')]) #CR: 0.67\n",
    "alpha(df[,c('self.evaluation_1', 'self.evaluation_2', 'self.evaluation_3')]) #CR: 0.78\n",
    "\n",
    "# calculate aggregated target and self-ratings\n",
    "df$target_rating = (df$other.evaluation_1 + df$other.evaluation_2 + df$other.evaluation_3)/3\n",
    "df$self_rating = (df$self.evaluation_1 + df$self.evaluation_2 + df$self.evaluation_3)/3\n",
    "\n",
    "# derive relative self-esteem (difference between target and self-ratings)\n",
    "df$difference_evaluation = df$target_rating - df$self_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Df Sum Sq Mean Sq F value Pr(>F)  \n",
      "condition         2   12.2   6.087   3.305 0.0374 *\n",
      "gender            1    4.6   4.612   2.504 0.1141  \n",
      "age               1    3.1   3.120   1.694 0.1936  \n",
      "ethnicity         4   10.7   2.671   1.450 0.2161  \n",
      "instagram_usage   1    4.9   4.913   2.667 0.1030  \n",
      "Residuals       582 1072.0   1.842                 \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
      "  Tukey multiple comparisons of means\n",
      "    95% family-wise confidence level\n",
      "\n",
      "Fit: aov(formula = difference_evaluation ~ condition + gender + age + ethnicity + instagram_usage, data = df)\n",
      "\n",
      "$condition\n",
      "                   diff        lwr          upr     p adj\n",
      "high-hidden  0.05117642 -0.2718321  0.374184944 0.9264655\n",
      "low-hidden  -0.27268164 -0.5975855  0.052222200 0.1200701\n",
      "low-high    -0.32385806 -0.6400208 -0.007695346 0.0432387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# there is a significant effect of the like count condition on relative self-esteem\n",
    "anova = aov(difference_evaluation ~ condition + gender + age + ethnicity + instagram_usage, data=df)\n",
    "print(summary(anova))\n",
    "\n",
    "TukeyHSD(anova, which = \"condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Df Sum Sq Mean Sq F value   Pr(>F)    \n",
      "variable              1  372.4   372.4 356.032  < 2e-16 ***\n",
      "condition             2    1.8     0.9   0.869  0.41977    \n",
      "gender                1    0.0     0.0   0.022  0.88265    \n",
      "age                   1    6.0     6.0   5.737  0.01677 *  \n",
      "ethnicity             4   16.0     4.0   3.831  0.00424 ** \n",
      "instagram_usage       1   48.8    48.8  46.636 1.37e-11 ***\n",
      "variable:condition    2    6.1     3.0   2.910  0.05486 .  \n",
      "Residuals          1171 1224.8     1.0                     \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "%%R \n",
    "# here we conduct a similar analysis but this time as a 2 (source: self or target) X 3 (likes: low, high, hidden) mixed-model ANOVA (Vogel et al., 2014)\n",
    "# this approach gives comparable results for the interaction between source and likes\n",
    "df_melt = melt(df, colnames(df)[-c(51,52)])\n",
    "anova_source = aov(value ~ variable + condition + variable:condition + gender + age + ethnicity + instagram_usage, data=df_melt)\n",
    "summary(anova_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Group.1 instagram_actions_1 instagram_actions_2 instagram_actions_3\n",
      "1  hidden            3.362162            1.686486            2.713514\n",
      "2    high            3.368932            1.635922            2.509709\n",
      "3     low            3.368159            1.716418            2.761194\n",
      "  instagram_actions_4\n",
      "1            1.870270\n",
      "2            1.781553\n",
      "3            2.049751\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# follow-up user behavior as a function of the like count\n",
    "aggregate(df[,c('instagram_actions_1', 'instagram_actions_2', 'instagram_actions_3', 'instagram_actions_4')]-53, list(df$condition), mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Df   Pillai approx F num Df den Df    Pr(>F)    \n",
      "condition         2 0.005771   0.5605      6   1162   0.76200    \n",
      "gender            1 0.029619   5.9012      3    580   0.00057 ***\n",
      "age               1 0.094243  20.1162      3    580 2.050e-12 ***\n",
      "ethnicity         4 0.071790   3.5672     12   1746 2.840e-05 ***\n",
      "instagram_usage   1 0.058748  12.0668      3    580 1.135e-07 ***\n",
      "Residuals       582                                              \n",
      "---\n",
      "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# the manipulation (condition) did not affect any of the dependent measures\n",
    "manova_results = manova(cbind(instagram_actions_1, instagram_actions_2, instagram_actions_4) ~ condition + gender + age + ethnicity + instagram_usage, data = df)\n",
    "summary(manova_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call:\n",
      "polr(formula = instagram_actions_4_fact ~ condition + gender + \n",
      "    age + ethnicity + instagram_usage + self_rating + target_rating, \n",
      "    data = df, Hess = TRUE, method = \"probit\")\n",
      "\n",
      "Coefficients:\n",
      "                   Value Std. Error t value\n",
      "conditionhigh   -0.08909    0.10786 -0.8260\n",
      "conditionlow     0.07798    0.10807  0.7216\n",
      "gender           0.08151    0.08741  0.9325\n",
      "age              0.03278    0.01232  2.6613\n",
      "ethnicity2       0.33974    0.13819  2.4585\n",
      "ethnicity3       0.05818    0.15940  0.3650\n",
      "ethnicity4      -0.11820    0.12859 -0.9192\n",
      "ethnicity5      -0.07347    0.31183 -0.2356\n",
      "instagram_usage  0.05618    0.03931  1.4290\n",
      "self_rating      0.38661    0.04185  9.2378\n",
      "target_rating   -0.01271    0.04856 -0.2617\n",
      "\n",
      "Intercepts:\n",
      "      Value   Std. Error t value\n",
      "53|54  1.9932  0.4381     4.5492\n",
      "54|55  2.7028  0.4407     6.1333\n",
      "55|56  3.1568  0.4432     7.1225\n",
      "56|57  3.7745  0.4491     8.4043\n",
      "57|58  4.2121  0.4558     9.2417\n",
      "58|59  4.8627  0.4695    10.3579\n",
      "\n",
      "Residual Deviance: 1967.76 \n",
      "AIC: 2001.76 \n",
      "          llh       llhNull            G2      McFadden          r2ML \n",
      "-9.838801e+02 -1.049692e+03  1.316236e+02  6.269629e-02  1.993546e-01 \n",
      "         r2CU \n",
      " 2.052731e-01 \n",
      "$ME.53\n",
      "                effect error t.value p.value\n",
      "conditionhigh    0.027 0.033   0.818   0.414\n",
      "conditionlow    -0.024 0.032  -0.729   0.467\n",
      "gender          -0.025 0.027  -0.932   0.352\n",
      "age             -0.010 0.004  -2.656   0.008\n",
      "ethnicity2      -0.093 0.034  -2.759   0.006\n",
      "ethnicity3      -0.017 0.047  -0.372   0.710\n",
      "ethnicity4       0.037 0.041   0.894   0.372\n",
      "ethnicity5       0.023 0.100   0.230   0.818\n",
      "instagram_usage -0.017 0.012  -1.428   0.154\n",
      "self_rating     -0.118 0.013  -8.888   0.000\n",
      "target_rating    0.004 0.015   0.262   0.794\n",
      "\n",
      "$ME.54\n",
      "                effect error t.value p.value\n",
      "conditionhigh    0.008 0.010   0.848   0.397\n",
      "conditionlow    -0.008 0.011  -0.700   0.484\n",
      "gender          -0.008 0.008  -0.924   0.356\n",
      "age             -0.003 0.001  -2.474   0.014\n",
      "ethnicity2      -0.040 0.020  -2.047   0.041\n",
      "ethnicity3      -0.006 0.017  -0.347   0.728\n",
      "ethnicity4       0.010 0.010   1.020   0.308\n",
      "ethnicity5       0.006 0.025   0.259   0.796\n",
      "instagram_usage -0.005 0.004  -1.397   0.163\n",
      "self_rating     -0.037 0.007  -5.515   0.000\n",
      "target_rating    0.001 0.005   0.261   0.794\n",
      "\n",
      "$ME.55\n",
      "                effect error t.value p.value\n",
      "conditionhigh   -0.003 0.004  -0.764   0.445\n",
      "conditionlow     0.003 0.003   0.754   0.451\n",
      "gender           0.003 0.003   0.902   0.367\n",
      "age              0.001 0.001   2.189   0.029\n",
      "ethnicity2       0.004 0.003   1.269   0.205\n",
      "ethnicity3       0.002 0.004   0.417   0.677\n",
      "ethnicity4      -0.005 0.006  -0.782   0.434\n",
      "ethnicity5      -0.003 0.014  -0.206   0.837\n",
      "instagram_usage  0.002 0.001   1.339   0.181\n",
      "self_rating      0.013 0.004   3.462   0.001\n",
      "target_rating    0.000 0.002  -0.261   0.794\n",
      "\n",
      "$ME.56\n",
      "                effect error t.value p.value\n",
      "conditionhigh   -0.012 0.015  -0.819   0.413\n",
      "conditionlow     0.010 0.014   0.726   0.468\n",
      "gender           0.011 0.012   0.929   0.353\n",
      "age              0.004 0.002   2.550   0.011\n",
      "ethnicity2       0.040 0.015   2.760   0.006\n",
      "ethnicity3       0.008 0.021   0.371   0.711\n",
      "ethnicity4      -0.016 0.018  -0.897   0.370\n",
      "ethnicity5      -0.010 0.043  -0.231   0.817\n",
      "instagram_usage  0.007 0.005   1.410   0.159\n",
      "self_rating      0.051 0.008   6.458   0.000\n",
      "target_rating   -0.002 0.006  -0.262   0.794\n",
      "\n",
      "$ME.57\n",
      "                effect error t.value p.value\n",
      "conditionhigh   -0.009 0.010  -0.827   0.408\n",
      "conditionlow     0.008 0.011   0.715   0.475\n",
      "gender           0.008 0.009   0.927   0.354\n",
      "age              0.003 0.001   2.489   0.013\n",
      "ethnicity2       0.035 0.015   2.295   0.022\n",
      "ethnicity3       0.006 0.016   0.361   0.718\n",
      "ethnicity4      -0.011 0.012  -0.933   0.351\n",
      "ethnicity5      -0.007 0.030  -0.240   0.810\n",
      "instagram_usage  0.006 0.004   1.400   0.162\n",
      "self_rating      0.038 0.007   5.813   0.000\n",
      "target_rating   -0.001 0.005  -0.262   0.794\n",
      "\n",
      "$ME.58\n",
      "                effect error t.value p.value\n",
      "conditionhigh   -0.008 0.010  -0.832   0.406\n",
      "conditionlow     0.007 0.010   0.708   0.479\n",
      "gender           0.007 0.008   0.923   0.356\n",
      "age              0.003 0.001   2.501   0.013\n",
      "ethnicity2       0.036 0.017   2.092   0.037\n",
      "ethnicity3       0.006 0.016   0.354   0.723\n",
      "ethnicity4      -0.010 0.011  -0.962   0.337\n",
      "ethnicity5      -0.006 0.026  -0.246   0.806\n",
      "instagram_usage  0.005 0.004   1.405   0.160\n",
      "self_rating      0.036 0.006   5.776   0.000\n",
      "target_rating   -0.001 0.004  -0.262   0.794\n",
      "\n",
      "$ME.59\n",
      "                effect error t.value p.value\n",
      "conditionhigh   -0.004 0.004  -0.833   0.405\n",
      "conditionlow     0.003 0.005   0.693   0.488\n",
      "gender           0.003 0.004   0.910   0.363\n",
      "age              0.001 0.001   2.318   0.021\n",
      "ethnicity2       0.018 0.010   1.801   0.072\n",
      "ethnicity3       0.003 0.007   0.346   0.729\n",
      "ethnicity4      -0.004 0.005  -0.980   0.328\n",
      "ethnicity5      -0.003 0.011  -0.254   0.799\n",
      "instagram_usage  0.002 0.002   1.368   0.172\n",
      "self_rating      0.016 0.004   4.049   0.000\n",
      "target_rating   -0.001 0.002  -0.261   0.794\n",
      "\n",
      "$ME.all\n",
      "                effect.53 effect.54 effect.55 effect.56 effect.57 effect.58\n",
      "conditionhigh       0.027     0.008    -0.003    -0.012    -0.009    -0.008\n",
      "conditionlow       -0.024    -0.008     0.003     0.010     0.008     0.007\n",
      "gender             -0.025    -0.008     0.003     0.011     0.008     0.007\n",
      "age                -0.010    -0.003     0.001     0.004     0.003     0.003\n",
      "ethnicity2         -0.093    -0.040     0.004     0.040     0.035     0.036\n",
      "ethnicity3         -0.017    -0.006     0.002     0.008     0.006     0.006\n",
      "ethnicity4          0.037     0.010    -0.005    -0.016    -0.011    -0.010\n",
      "ethnicity5          0.023     0.006    -0.003    -0.010    -0.007    -0.006\n",
      "instagram_usage    -0.017    -0.005     0.002     0.007     0.006     0.005\n",
      "self_rating        -0.118    -0.037     0.013     0.051     0.038     0.036\n",
      "target_rating       0.004     0.001     0.000    -0.002    -0.001    -0.001\n",
      "                effect.59\n",
      "conditionhigh      -0.004\n",
      "conditionlow        0.003\n",
      "gender              0.003\n",
      "age                 0.001\n",
      "ethnicity2          0.018\n",
      "ethnicity3          0.003\n",
      "ethnicity4         -0.004\n",
      "ethnicity5         -0.003\n",
      "instagram_usage     0.002\n",
      "self_rating         0.016\n",
      "target_rating      -0.001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# ordered probit regression - posting frequency (example)\n",
    "# note that these type of models are prefered given the ordinal scale of the answer response options (yet a simple lm-model gives comparable results)\n",
    "df$instagram_actions_4_fact = as.factor(df$instagram_actions_4)\n",
    "post_probit = polr(instagram_actions_4_fact ~ condition + gender + age + ethnicity + instagram_usage + self_rating + target_rating, data = df, Hess = TRUE, method='probit')\n",
    "print(summary(post_probit))\n",
    "print(pR2(post_probit))\n",
    "ocME(post_probit)$out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/instagram_header.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Klaasse Bos, R.J. (2020). Web Appendix: Goodbye Likes, Hello Mental Health: How Hiding Like Counts Affects User Behavior & Self-Esteem.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
