{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/instagram_header.png\" align=\"left\" style=\"margin-bottom: 20px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Web Appendix - Data Collection & Preparation </h2>\n",
    "\n",
    "<p style=\"clear: both;\">This online appendix complements the master thesis \"Goodbye Likes, Hello Mental Health: How Hiding Like Counts Affects User Behavior & Self-Esteem\":</p> \n",
    "\n",
    "<p><i>Likes are widely available on social network services and are known to influence people’s self-image. An emerging literature has started to look at potential detrimental effects of social media use among teenagers. We study how Instagram users’ posting frequency, variety, like behavior, and relative self-esteem are affected by an intervention in which like counts were hidden in selected treatment countries. Using a unique panel data set of individual users’ Instagram posts across multiple years, we find evidence that users posted more frequently and more varied than in the months prior to the intervention. On the other hand, the number of likes decreases as people are no longer influenced by others’ evaluations, especially among users with a small following. Further, in an experiment we show that the number of likes people see on others’ posts affects their relative self-esteem, and that users are more likely to self-disclose once they rate themselves more positively. These results are critical to understanding the dynamics on visual-based social media in order to foster a healthy online environment.</i></p>\n",
    "\n",
    "<p>In this notebook, we perform the following steps (run this .ipynb-file locally for clickable anchors): </p>\n",
    "\n",
    "A. [Instagram Influencer Seed](#instagram-influencer-seed)  \n",
    "B. [Instagram Consumers Selection](#instagram-consumers-selection)  \n",
    "C. [Collect & Preprocess Instagram Data](#preprocess-instagram-data)  \n",
    "D. [Computer Vision](#computer-vision)  \n",
    "E. [Cosine Similarity & Image Similarity](#cosine-similarity)  \n",
    "F. [Outlier Screening](#outlier-screening)  \n",
    "G. [Propensity Score Matching](#propensity-score-matching)  \n",
    "\n",
    "<i> Note: Steps H and I (data analysis) can be found over <a href=\"https://github.com/RoyKlaasseBos/Hiding-Instagram-Likes/blob/master/Web_Appendix_Data_Analysis.ipynb\">here</a>. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, psycopg2, pickle, pandas as pd, requests, json\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# define paths to PostgresSQL database (instructions on how to setup the environment variables can be found in the README file)\n",
    "host = os.environ['INSTAGRAM_DB_URL'] \n",
    "password = os.environ['INSTAGRAM_DB_KEY']\n",
    "connection = psycopg2.connect(host=host, user='postgres', password=password, dbname='postgres');\n",
    "cursor = connection.cursor()\n",
    "connection_string = 'postgresql+psycopg2://postgres:' + password + '@' + host + '/postgres'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# support R in Jupyter Notebook\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: DBI\n",
      "\n",
      "R[write to console]: Loading required package: MASS\n",
      "\n",
      "R[write to console]: ## \n",
      "##  Matching (Version 4.9-6, Build Date: 2019-04-07)\n",
      "##  See http://sekhon.berkeley.edu/matching for additional documentation.\n",
      "##  Please cite software as:\n",
      "##   Jasjeet S. Sekhon. 2011. ``Multivariate and Propensity Score Matching\n",
      "##   Software with Automated Balance Optimization: The Matching package for R.''\n",
      "##   Journal of Statistical Software, 42(7): 1-52. \n",
      "##\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "# in case you are unable to select a CRAN mirror in Jupyter Notebook: open RStudio and install the packages there which resolves the issue.\n",
    "install.packages(c(\"RPostgreSQL\", \"Matching\"))\n",
    "library(RPostgreSQL)\n",
    "library(Matching)\n",
    "\n",
    "host = Sys.getenv(c(\"INSTAGRAM_DB_URL\"))\n",
    "password = Sys.getenv(c(\"INSTAGRAM_DB_KEY\"))\n",
    "\n",
    "drv = dbDriver(\"PostgreSQL\")\n",
    "con = dbConnect(drv, host=host, \n",
    "                port='5432', dbname='postgres',\n",
    "                user='postgres', password=password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"instagram-influencer-seed\"></a>\n",
    "### A. Instagram Influencer Seed\n",
    "\n",
    "<a href=\"https://hypeauditor.com/top-instagram/\">HypeAuditor</a> lists the top influencers by  category by country. The ranking is updated daily and takes into account quality audience and authentic engagement to control for bots and inactive accounts. For each listing the number of followers from a given country and the total number of followers are indicated. First, we scraped all listings for each category and country combination in February 2020. The number of listings for each combination varies depending on the volume of influencers in the domain. In 45 cases (0.3%) the ``followers_from_country``  column was missing because the data was unavailable on HypeAuditor. These records have been excluded from our analysis. Second, we divide both metrics by one another to derive the *purity*. That is, the percentage of influencers' followers from a given country. Third, we exclude followers whose purity is below 50%. As a result, we end up with a list of influencers (N = 5391) whose main following base is located in a single country (41% of all top influencers). Fourth, we sort the influencers by purity and pick the top 20 influencers for each country in our dataset.\n",
    "\n",
    "<img src=\"./images/hypeauditor.png\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(table):\n",
    "    '''pull in all data from a table'''\n",
    "    cursor.execute(\"SELECT * FROM \" + table )\n",
    "    data = pd.DataFrame(cursor.fetchall())\n",
    "    data.columns = [desc[0] for desc in cursor.description]\n",
    "    return data\n",
    "\n",
    "def pickle_files(pickle_name, df):\n",
    "    '''store output of data frame as pickle'''\n",
    "    with open(pickle_name, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "        \n",
    "def convert_follower_counts(df, column):\n",
    "    '''convert string follower counts (e.g., 3K) into numeric (e.g., 3000)'''\n",
    "    for counter in range(len(df)): \n",
    "        if \"M\" in df.loc[counter, column]:\n",
    "            df.loc[counter, column] = float(df.loc[counter, column].replace(\"M\", \"\")) * 1000000        \n",
    "        elif \"K\" in df.loc[counter, column]:\n",
    "            df.loc[counter, column] = float(df.loc[counter, column].replace(\"K\", \"\")) * 1000\n",
    "    return df\n",
    "\n",
    "def extract_country(df): \n",
    "    '''extract country from URL (e.g., https://hypeauditor.com/top-instagram-beer-wine-spirits-brazil/ --> brazil)'''\n",
    "    countries = [\"australia\", \"brazil\", \"canada\", \"china\", \"france\", \"germany\", \"hong-kong\", \"india\", \"indonesia\", \\\n",
    "                 \"italy\", \"malaysia\", \"mexico\", \"russia\", \"saudi-arabia\", \"slovakia\", \"spain\", \"switzerland\", \"ukraine\", \\\n",
    "                 \"united-arab-emirates\", \"united-kingdom\", \"united-states\"]\n",
    "    for counter in range(len(df)): \n",
    "        for country in countries: \n",
    "            if country in df.loc[counter, 'url']:\n",
    "                df.loc[counter, 'country'] = country\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_purity(df):\n",
    "    '''determine the percentage of influencers' followers from a given country'''\n",
    "    for counter in range(len(df)):\n",
    "        df.loc[counter, 'percentage_country'] = float(df.loc[counter, 'followers_from_country']) / float(df.loc[counter,'total_follower'])\n",
    "    return df\n",
    "\n",
    "\n",
    "# import data\n",
    "df = load_data(\"hypeauditor\")\n",
    "\n",
    "# add country column to data\n",
    "df = extract_country(df)\n",
    "\n",
    "# convert follower counts to numeric\n",
    "df = convert_follower_counts(df, 'followers_from_country')\n",
    "df = convert_follower_counts(df, 'total_follower')\n",
    "\n",
    "# exclude records for which either the followers from country or total follower count is missing\n",
    "df = df[(df['followers_from_country'] != \"\") & (df['total_follower'] != \"\")].reset_index(drop=True)\n",
    "\n",
    "# add purity measure\n",
    "df = calculate_purity(df)\n",
    "\n",
    "# exclude influencers whose purity is below 50%; mean purity increases from 37.1% to 70.1%\n",
    "df = df.loc[df.percentage_country > .5]\n",
    "\n",
    "# select top influencers from each country whose purity is highest (mean purity: 82.8%)\n",
    "top_20 = df.groupby('country')['percentage_country'].nlargest(20)\n",
    "\n",
    "# obtain the usernames belonging to these influencers\n",
    "indices = [top_20.index[counter][1] for counter in range(len(top_20.index))] \n",
    "selected_df = df.loc[indices, ['username', 'country', 'percentage_country']]\n",
    "# selected_df.to_sql('influencer_country_purity', engine, index=None, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"instagram-consumers-selection\"> </a>\n",
    "### B. Instagram Consumers Selection\n",
    "\n",
    "For each selected influencer we collect a list of their followers using [Phantombuster](https://phantombuster.com/automations/instagram/7085/instagram-profile-scraper) (figure below), draw a random sample of  followers, and validate their country of origin in order to construct our dataset of consumers. Our sample includes personal accounts owned by individual users who do not engage in commercial activities on Instagram and whose country of origin is validated. Furthermore, we validate the user's country of origin. The step-by-step user screening procedure with examples can be found [here](https://github.com/RoyKlaasseBos/Hiding-Instagram-Likes/blob/master/User_Screening_Manual.pdf). Finally, we end up with a list of 40 accounts by country:</p>\n",
    "\n",
    "\n",
    "| Country | Type | #Accounts |\n",
    "| ------ | ------ |------ |\n",
    "| Australia | Treatment | 40 |\n",
    "| Canada | Treatment | 40 |\n",
    "| France | Control | 40 |\n",
    "| Germany | Control | 40 |\n",
    "| Italy | Treatment | 40 |\n",
    "| Netherlands | Control | 40 |\n",
    "| Spain | Control | 40 |\n",
    "| United Kingdom | Control | 40 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocess-instagram-data\"></a>\n",
    "### C. Collect & Preprocess Instagram Data\n",
    "In step 1 and 2 we created a list of Instagram usernames of which we collected historical post data using [Instagram Scraper](https://github.com/arc298/instagram-scraper). This is a command-line application written in Python to obtain user information, social relationship information, and photo information. Since the photo and video files attached to each post take up a significant amount of memory, we only store the links to the online media files. More specifically, we run the command below to collect the post information of usernames in `FILE_NAME.txt` (i.e., text file that contains all usernames in our sample). \n",
    "\n",
    "<img src=\"./images/instagram_scraper.png\" alt=\"Instagram Scraper Github\" align=\"left\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`instagram-scraper -f FILE_NAME.txt --media-types none --media-metadata --profile-metadata -T {username}_{urlname}`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraping process yields a separate JSON-file for each account which requires further preprocessing for follow-up analysis. For each user we extracted post and user level data and stored it into a dataframe which we then pushed to a local database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_posts(df):\n",
    "    '''parse the json file for each username in the text file and store preprocessed records in a data frame'''\n",
    "    \n",
    "    # declare dataframes for post data (posts) and arrays for user accounts which are private (i.e., user data could not be scraped)\n",
    "    posts = pd.DataFrame()\n",
    "    private = []\n",
    "\n",
    "    # test if user profile is publicly available\n",
    "    for user in range(len(df)):\n",
    "        d = df.loc[user, 'json']\n",
    "          \n",
    "        try:        \n",
    "            # post level data\n",
    "            shortcode = [d[counter]['shortcode'] for counter in range(len(d))]\n",
    "            description = [d[counter]['edge_media_to_caption']['edges'][0]['node']['text'] if len(d[counter]['edge_media_to_caption']['edges']) > 0 else \"NA\" for counter in range(len(d))]\n",
    "            total_likes = [d[counter]['edge_media_preview_like']['count'] for counter in range(len(d))]\n",
    "            total_comments = [d[counter]['edge_media_to_comment']['count'] for counter in range(len(d))]\n",
    "            hashtags = [d[counter]['tags'] if type(d[counter].get('tags')) == list else \"NA\" for counter in range(len(d))]\n",
    "            content_type = [d[counter]['__typename'] for counter in range(len(d))]\n",
    "            timestamp = [d[counter]['taken_at_timestamp'] for counter in range(len(d))]\n",
    "            video_views = [d[counter]['video_view_count'] if type(d[counter].get('video_view_count')) == int else 0 for counter in range(len(d))]\n",
    "            media1 = [d[counter]['urls'][0] for counter in range(len(d))]\n",
    "\n",
    "            posts_temp = pd.DataFrame({\n",
    "                           \"username\": d[0]['username'],\n",
    "                           \"shortcode\": shortcode, \n",
    "                           \"description\": description, \n",
    "                           \"total_likes\": total_likes,\n",
    "                           \"total_comments\": total_comments,\n",
    "                           \"hashtags\": hashtags,\n",
    "                           \"content_type\": content_type,\n",
    "                           \"timestamp\": timestamp, \n",
    "                           \"video_views\": video_views,\n",
    "                           \"media1\": media1 # for image caroussels we focused on the first photo/video\n",
    "                          })\n",
    "\n",
    "            posts = pd.concat([posts_temp, posts]).reset_index(drop=True)\n",
    "            \n",
    "        except: \n",
    "            private.append(user)\n",
    "\n",
    "    # convert epoch time to regular timestamp\n",
    "    posts['timestamp'] = pd.to_datetime(posts['timestamp'], unit='s')\n",
    "\n",
    "    # add regular date (without time)\n",
    "    posts['date'] = posts['timestamp'].dt.date\n",
    "\n",
    "    return posts, private\n",
    "\n",
    "\n",
    "def parse_profiles(df):\n",
    "    '''parse the personal biography for each username in the text file and store preprocessed records in a data frame'''\n",
    "    profile = pd.DataFrame()\n",
    "\n",
    "    # test if user profile is publicly available\n",
    "    for user in range(len(df)):\n",
    "        d = df.loc[user, 'json']\n",
    "\n",
    "        # user level profile data\n",
    "        try: \n",
    "            followers_count = d['info']['followers_count']\n",
    "            following_count = d['info']['following_count']\n",
    "            posts_count = d['info']['posts_count']\n",
    "            biography = d['info']['biography']\n",
    "            full_name = d['info']['full_name']\n",
    "\n",
    "            profile_temp = pd.DataFrame({\n",
    "                \"username\": d['username'][0],\n",
    "                \"followers_count\": followers_count,  \n",
    "                \"following_count\": following_count, \n",
    "                \"posts_count\": posts_count,\n",
    "                \"biography\": biography, \n",
    "                \"full_name\": full_name \n",
    "            })\n",
    "            \n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        profile = pd.concat([profile_temp, profile]).reset_index(drop=True)\n",
    "\n",
    "    return profile\n",
    "\n",
    "# for all usernames process JSON files and push normalized data to SQL database (takes a while to run!)\n",
    "temp_output = parse_posts(load_data('consumers_posts_json'))\n",
    "temp_posts = temp_output[0]\n",
    "temp_private = temp_output[1]\n",
    "temp_profile = parse_profiles(load_data('consumers_profile_json'))\n",
    "        \n",
    "#temp_posts.to_sql('consumers_posts', engine, index=None, if_exists='replace')\n",
    "#temp_profile.to_sql('consumers_profile', engine, index=None, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"computer-vision\"></a>\n",
    "### D. Computer Vision\n",
    "We use Azure Cognitive Services Computer Vision Application Programming Interface ([API](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/)) to analyze image content. For every image, the API returns a vector of tags and confidence scores (figure below). First, we make an API request and pickle all output data for further analysis. Second, we compute image similarity within and between-subjects using image tags data. Note that Instagram image URLs are valid for a limited amount of time. The code sample below, therefore, only runs for recently scraped data. Expired URLs are printed in the console.\n",
    "\n",
    "<img src=\"./images/vision_api_example.png\" align=\"left\" alt=\"Computer Vision API Example (tags)\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'COMPUTER_VISION_SUBSCRIPTION_KEY' in os.environ:\n",
    "    subscription_key = os.environ['COMPUTER_VISION_SUBSCRIPTION_KEY']\n",
    "else:\n",
    "    print(\"\\nSet the COMPUTER_VISION_SUBSCRIPTION_KEY environment variable.\\n**Restart your shell or IDE for changes to take effect.**\")\n",
    "    sys.exit()\n",
    "\n",
    "if 'COMPUTER_VISION_ENDPOINT' in os.environ:\n",
    "    endpoint = os.environ['COMPUTER_VISION_ENDPOINT']\n",
    "    \n",
    "analyze_url = endpoint + \"vision/v2.1/analyze\"\n",
    "\n",
    "\n",
    "def process_image(temp_df, categories=True):\n",
    "    '''obtain categories or tags image data from all images in dataframe using Azure Cognitive Services'''\n",
    "    df = pd.DataFrame(columns=['uri', 'timestamp', 'category', 'score'])\n",
    "    \n",
    "    for counter in range(len(temp_df)):\n",
    "        image_url = temp_df.loc[counter, 'media1']\n",
    "        time_stamp = temp_df.loc[counter, 'timestamp']\n",
    "    \n",
    "        headers = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "        data = {'url': image_url}\n",
    "        params = {'visualFeatures': 'Categories'} if categories else {'visualFeatures': 'Tags'}\n",
    "        \n",
    "        try: \n",
    "            response = requests.post(analyze_url, headers=headers,\n",
    "                                 params=params, json=data)\n",
    "            output = response.json()\n",
    "\n",
    "            if categories: \n",
    "                for category in output['categories']: \n",
    "                    df = df.append(\n",
    "                        dict(\n",
    "                            uri = image_url,\n",
    "                            timestamp = time_stamp,\n",
    "                            category = category['name'],\n",
    "                            score = category['score'],\n",
    "                        ), ignore_index = True)\n",
    "\n",
    "            else: \n",
    "                for tag in output['tags']: \n",
    "                    df = df.append(\n",
    "                            dict(\n",
    "                                uri = image_url,\n",
    "                                timestamp = time_stamp,\n",
    "                                category = tag['name'],\n",
    "                                score = tag['confidence'],\n",
    "                            ), ignore_index=True)           \n",
    "                    \n",
    "        except: \n",
    "            #image url expired\n",
    "            print(image_url)             \n",
    "            \n",
    "    return df\n",
    "\n",
    "# to save time and computing resources we only collect image tags data among users selected after matching\n",
    "# in step 7 we describe the propensity score matching procedure\n",
    "consumers_psm_query = load_data(\"consumers_psm\")\n",
    "consumers_selected = consumers_psm_query['username']\n",
    "\n",
    "# consumers' post level data\n",
    "temp_posts = load_data(\"consumers_posts\")\n",
    "\n",
    "# The image urls in our sample are expired which implies we cannot access nor analyze these images anymore \n",
    "\n",
    "# for consumer in consumers_selected:\n",
    "#     if not os.path.isfile('./pickles/image_output/Azure_Tags/' + consumer + '.pickle'):\n",
    "#         temp = temp_posts.loc[(temp_posts.username == consumer) & (temp_posts.content_type != \"GraphVideo\")].reset_index(drop=True) \n",
    "#         image_tags = process_image(temp, False)\n",
    "#         image_tags.to_sql('image_tags', con=engine, index=None, if_exists='append') \n",
    "#         pickle_files('pickles/image_output/Azure_Tags/' + consumer + '.pickle', image_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cosine-similarity'> </a>\n",
    "### E. Cosine Similarity & Image Similarity\n",
    "\n",
    "#### E.1 Cosine Similarity\n",
    "To illustrate how the cosine similarity scores are derived, we go over a fictitious example for the within-subject design. Let's assume a user posted two pictures of which we want to compute the cosine similarity: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cosine_similarity.jpg\" align=\"left\" alt=\"Cosine Similarity Example\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As follows from the figure the computer vision algorithm API returned three tags for both pictures. The first picture contains a group of people watching the sunset together, and the second picture also shows a group of people standing in a forest. To account for uncertainty each of these tags is associated with a confidence score, which we can write down in matrix notation as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_group</th>\n",
       "      <th>outdoor</th>\n",
       "      <th>sunset</th>\n",
       "      <th>forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>picture1.jpg</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picture2.jpg</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              people_group  outdoor  sunset  forest\n",
       "picture1.jpg          0.67     0.93    0.89    0.00\n",
       "picture2.jpg          0.74     0.92    0.00    0.88"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pictures = pd.DataFrame([[0.67, 0.93, 0.89, 0.00], [0.74, 0.92, 0.00, 0.88]], columns=['people_group', 'outdoor', 'sunset', 'forest'], index=['picture1.jpg', 'picture2.jpg'])\n",
    "pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first and second row `forest` and `sunset` were assigned a confidence score of `0.00` respectively as these tags were not present in the images. Next, we perform the cosine similarity operation which measures the angle between two vectors and determines whether two vectors are pointing in the same direction. More specifically, we multiply the confidence scores of pictures 1 and 2 for each image tag (e.g., for people: 0.67 x 0.74) and divide by the multiplication of the length of both vectors. Mathematically, this can be denoted as: \n",
    "$$sim(r,c)=  (r \\cdot c)/(\\left\\Vert r \\right\\Vert \\cdot \\left\\Vert c \\right\\Vert)$$ \n",
    "<br /> \n",
    "Here $r$ and $c$ are the image vectors for picture 1 and 2 respectively, and $||r||$ is defined as $\\sqrt{r_1^2+r_2^2+ ... + r_n^2}$. A larger confidence score has a larger weight and more overlapping image tags gives a higher cosine similarity score. Filling in the confidence scores above, we find a cosine similarity of `0.63` between picture 1 and 2. The diagonal contains 1s as comparing any image with itself always yields a cosine similarity of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.63240507],\n",
       "       [0.63240507, 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we explain how we can apply cosine similarity transformations to address whether the variety of posts changes after the introduction of the intervention. First, we compute the cosine similarity between pictures taken before [after] the intervention with all other pictures taken before [after] the intervention. This gives a mean image similarity score by user: \n",
    "<br/>\n",
    "\n",
    "| username | before_after | image_similarity |\n",
    "| -------  | -------- | --------- | \n",
    "| alexanderkuckart | before | 0.2140 | \n",
    "| alexanderkuckart | after | 0.1984 | \n",
    "| ... | ... | ... | \n",
    "| xannabellex27 | before | 0.3134 | \n",
    "| xannabellex27 | after | 0.3087 | \n",
    "\n",
    "Again, let's consider a hypothethical user who used to only share like-seeking selfies on Instagram. After hiding like counts about half of the posts still include selfies, but the remaining posts include other subjects (e.g. scenery). This implies that the image similarity would drop since the cosine similarity of a blend of selfies and scenery photos is lower than the cosine similarity among a homogeneous sample of selfies. \n",
    "\n",
    "#### E.2 Image Similarity\n",
    "\n",
    "First, we make within-subject comparisons to address whether the variety of posts changes after the introduction of the intervention. Second, we make between-subjects comparisons to determine whether treated users share more unique content relative to others. \n",
    "\n",
    "*Within-subject similarity*  \n",
    "For each user i we compute the cosine similarity between pictures taken by the same user $i$. We distinguish between pictures taken before ($1_{before}$…$n_{before}$) and after  \n",
    "($1_{after}$…$n_{after}$) the intervention. This yields a similarity matrix in which each picture *before* [*after*] the intervention is compared with all pictures *before* [*after*] hiding like counts (i.e., white squares in figure below). Each row ($r$) and column ($c$) name present a picture from user $i$ in $k$, where $k$ can take on the value *before* or *after*. Given these two separate subsets $k$, we calculate how similar each picture on average is to all other pictures in the same subset. That is, for each row we take the row average excluding the diagonal values. Finally, we aggregate the results across all rows in $k$:\n",
    "\n",
    "\n",
    "$\\omega_{ik} = \\frac{1}{n_{ik}(n_{ik}-1)}\\sum_{r=1}^{n} \\sum_{c=1|c \\neq r}^{i-1} sim(r_{ik},c_{ik})$ \n",
    "\n",
    "The row and column names represent pictures before and after the intervention for user *i* ($\\mu_i$). Values in the matrix denote the cosine similarity for each picture pair (only the diagonal of 1s have been reported).  For the purpose of this analysis we restrict ourselves to the white squares in the top left and bottom right quadrant of the figure. Within these areas we compute row means (excluding the diagonal values) to determine how similar a given picture is to all other pictures in the same subset on average. Thereafter, we derive the before and after within-subject similarity by taking the average of the row means in the top left and bottom right squares, respectively. Note: calculating column means, rather than row means, yields identical outcomes. \n",
    "\n",
    "<img src=\"./images/within_subjects_similarity.png\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def within_subject_similarity(consumers, consumers_selected):\n",
    "    '''compute the within-subject similarity from image tags before and after the intervention'''\n",
    "\n",
    "    for consumer in consumers_selected:\n",
    "        w_similarity_scores = pd.DataFrame() \n",
    "        \n",
    "        image_data = load_data(\"image_tags WHERE username = '\" + consumer + \"'\")\n",
    "            \n",
    "        consumers_df = pd.merge(image_data, consumers, left_on='uri', right_on='media1')[['uri', 'category', 'score', 'before_after']]\n",
    "\n",
    "        # turn image categories/tags into a matrix (rows: images, columns: categories/tags) and order by date (year & month)\n",
    "        tags_matrix = consumers_df.pivot_table(index=[\"before_after\", \"uri\"], columns=\"category\")\n",
    "        tags_matrix = tags_matrix.fillna(0)\n",
    "        similarity = cosine_similarity(tags_matrix)\n",
    "\n",
    "        before_intervention = len(tags_matrix.loc[('before')])\n",
    "        after_intervention = len(tags_matrix.loc[('after')])\n",
    "\n",
    "        before_similarity = pd.Series([similarity[counter][list(range(0, counter)) + list(range(counter + 1, before_intervention))].mean() for counter in range(before_intervention)]).mean()\n",
    "        w_similarity_scores.loc[len(w_similarity_scores) + 1, ['username', 'before_after', 'image_similarity']] = [consumer, 'before', before_similarity]\n",
    "\n",
    "        after_similarity = pd.Series([similarity[counter][list(range(before_intervention, counter)) + list(range(counter + 1, before_intervention + after_intervention))].mean() for counter in range(before_intervention, before_intervention + after_intervention)]).mean()\n",
    "        w_similarity_scores.loc[len(w_similarity_scores) + 1, ['username', 'before_after', 'image_similarity']] = [consumer, 'after', after_similarity]\n",
    "\n",
    "        #w_similarity_scores.to_sql(\"image_similarity_within_tags\", engine, index=None, if_exists='append')    \n",
    "\n",
    "# load consumers data and extract year and month from dates            \n",
    "consumers_before_after_query = \"SELECT c.username, media1, \\\n",
    "CASE WHEN cc.country = 'canada' AND c.date > '2019-04-30' THEN 'after' \\\n",
    "WHEN c.date > '2019-07-17' THEN 'after' \\\n",
    "ELSE 'before' END as before_after \\\n",
    "FROM consumers_posts c \\\n",
    "INNER JOIN consumers_psm cp ON c.username = cp.username \\\n",
    "INNER JOIN consumers_country cc ON c.username = cc.username \\\n",
    "WHERE c.date > '2018-04-30' AND c.date < '2020-04-30'\"\n",
    "\n",
    "cursor.execute(consumers_before_after_query)\n",
    "consumers_before_after = pd.DataFrame(cursor.fetchall())\n",
    "consumers_before_after.columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "# compute within-subject similarity \n",
    "within_subject_similarity(consumers_before_after, consumers_before_after['username'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Between-subjects similarity*  \n",
    "To assess between-subjects similarity (B) we distinguish between cohorts of users in the treatment and control group. We choose for these comparisons for two reasons. First, Instagram users may especially stay on top of the trends in their local market and therefore their postings might have already been more like other treatment units prior to the intervention. Second, by defining cohorts we establish more homogeneous clusters of users. Within these two cohorts, we determine the cosine similarity of each user pair ($u_i, u_j$) in k = {before, after} (i.e., white squares in figure below). That is, how similar pictures from user i are to pictures from another user j on average, where $u_i$  and $u_j$  belong to the same cohort.\n",
    "\n",
    "$B_{ijk} = \\frac{1}{n_i n_j} \\sum_{r=1}^{n} \\sum_{c=1}^{n} sim(r_{ijk},c_{ijk})$\n",
    "\n",
    "The row and column names represent pictures before the intervention for user i ($u_i$) and user j ($u_j$) in the same cohort. Values in the matrix denote the cosine similarity for each picture pair (note: values are left out for simplicity). For the purpose of this analysis we restrict ourselves to the top right or the bottom left white square. Within this area we compute row means to determine how similar a given picture from $u_i$ [$u_j$] is to all pictures from $u_j$ [$u_i$] on average. Thereafter, we sum up the row means and divide by the number of rows ($n_{ik}$ [$n_{jk}$]) to derive the before between-subjects similarity. In a similar fashion, the after between-subjects similarity can be determined. Note that only one of both white squares should be used to avoid duplicates.\n",
    "\n",
    "<img src=\"./images/between_subjects_similarity.png\" width=\"500px\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_image_output(consumer, consumers, before_after='before'):\n",
    "    '''construct cosine similarity matrix for either all images before or after the intervention'''\n",
    "    image_input = load_data(\"image_tags WHERE username = '\" + consumer + \"'\")\n",
    "    consumers_df = pd.merge(image_input, consumers, left_on='uri', right_on='media1')[['uri', 'category', 'score', 'before_after', 'treatment_control']]\n",
    "    tags_matrix = consumers_df.pivot_table(index=[\"before_after\", \"uri\"], columns=\"category\")\n",
    "    tags_matrix = tags_matrix.fillna(0)\n",
    "    if before_after == 'before':\n",
    "        tags_matrix = tags_matrix.loc[('before')]\n",
    "    else: \n",
    "        tags_matrix = tags_matrix.loc[('after')]    \n",
    "    return tags_matrix\n",
    "\n",
    "\n",
    "def between_subjects_similarity(consumers):\n",
    "    '''compute the between-subjects similarity from image tags before and after the intervention'''\n",
    "    for consumer1 in consumers.username.unique():\n",
    "        for consumer2 in consumers.username.unique(): \n",
    "            b_similarity_scores = pd.DataFrame(columns = ['username1' , 'username2', 'username1_2', 'before_after', 'similarity']) \n",
    "           \n",
    "            try: \n",
    "                if consumer1 != consumer2: # do not compare image data of the same user\n",
    "                    for before_after in ['before', 'after']: # run this procedure for images before and after the intervention separately\n",
    "                        consumer1_df = create_image_output(consumer1, consumers, before_after)\n",
    "                        consumer2_df = create_image_output(consumer2, consumers, before_after)\n",
    "                        consumers_1_2 = pd.concat([consumer1_df, consumer2_df])\n",
    "                        consumers_1_2 = consumers_1_2.fillna(0)\n",
    "                        similarity = cosine_similarity(consumers_1_2)\n",
    "\n",
    "                        # for each image of consumer 1 take the mean cosine similarity with all image of consumer 2\n",
    "                        comparisons = [similarity[counter][len(consumer1_df):].mean() for counter in range(len(consumer1_df))]\n",
    "\n",
    "                        # aggregate the results across all images of consumer 1 (so take the mean of all mean cosine similarities) \n",
    "                        before_after_similarity = pd.Series(comparisons).mean() \n",
    "                        b_similarity_scores = b_similarity_scores.append({\n",
    "                                                                \"username1\": consumer1,\n",
    "                                                                \"username2\": consumer2,\n",
    "                                                                \"username1_2\": \"_\".join(sorted([consumer1, consumer2])),\n",
    "                                                                \"before_after\": before_after, \n",
    "                                                                \"similarity\": before_after_similarity\n",
    "                                                                }, ignore_index=True)\n",
    "\n",
    "                #b_similarity_scores.to_sql(\"image_similarity_between_tags\", engine, index=None, if_exists='append')\n",
    "\n",
    "            except: \n",
    "                pass        \n",
    "\n",
    "# load consumers data and extract year and month from dates            \n",
    "connection = engine.connect()\n",
    "consumers_before_after_treatment_control_query = \"SELECT c.username, media1, CASE WHEN cc.country = 'canada' AND c.date > '2019-04-30' THEN 'after' WHEN c.date > '2019-07-17' THEN 'after' ELSE 'before' END as before_after, CASE WHEN cc.country IN ('australia', 'canada', 'italy') THEN 'treatment' ELSE 'control' END as treatment_control FROM consumers_posts c INNER JOIN consumers_psm cp ON c.username = cp.username INNER JOIN consumers_country cc ON c.username = cc.username WHERE c.date > '2018-04-30' AND c.date < '2020-04-30'\"\n",
    "consumers_before_after_treatment_control = connection.execute(consumers_before_after_treatment_control_query).fetchall()\n",
    "consumers = pd.DataFrame(consumers_before_after_treatment_control).rename({0: 'username', 1: 'media1', 2: 'before_after', 3: 'treatment_control'}, axis=1)\n",
    "\n",
    "# compute between-subjects similarity (takes a while to run!)\n",
    "# between_subjects_similarity(consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='outlier-screening'></a>\n",
    "### F. Outlier Screening\n",
    "Even though we use a stringent [Instagram consumers selection](#instagram-consumers-selection) procedure, it may sporadically occur that a user systematically differs in terms of the number of followers, followings, and the average number of likes of their image posts. To overcome this issue we use a multivariate outlier screening approach and remove these users from our sample before propensity score matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# split query into subqueries\n",
    "query_1 = \"SELECT x.username, CAST(followers_count AS NUMERIC), CAST(following_count AS NUMERIC), average_likes FROM\" \n",
    "query_2 = \"x INNER JOIN (SELECT username, AVG(total_likes) as average_likes FROM\"\n",
    "query_3 = \"l GROUP BY username) l ON x.username = l.username\"\n",
    "\n",
    "query_user_data = function(user_profile, user){\n",
    "    # paste queries and determine follower count, following count, and average number of likes by user\n",
    "    query = paste(query_1, user_profile, query_2, user, query_3)\n",
    "    return(dbGetQuery(con, query))\n",
    "}\n",
    "\n",
    "outlier_screening = function(df){\n",
    "    # determine if the mahalanobis distance exceeds the threshold value\n",
    "    mahal = mahalanobis(df[,-1], colMeans(df[,-1]), cov(df[,-1]), tol=1e-20)\n",
    "    cutoff = qchisq(1-0.001, ncol(df[,-1]))\n",
    "    outliers = subset(df, mahal > cutoff)    \n",
    "    no_outliers = subset(df, mahal < cutoff)\n",
    "    return(c(outliers, no_outliers))\n",
    "}\n",
    "\n",
    "\n",
    "remove_record = function(table, usernames){\n",
    "    # remove all posts of users labeled as outliers\n",
    "    for(username in usernames){\n",
    "        del_query = paste(\"DELETE FROM \", table, \" WHERE username='\", username, \"'\", sep=\"\")\n",
    "        dbGetQuery(con, del_query)\n",
    "    }\n",
    "}\n",
    "\n",
    "# collect user stats and then screen for outliers\n",
    "consumers_stats = query_user_data(\"consumers_profile\", \"consumers_posts\")\n",
    "consumers_screening = outlier_screening(consumers_stats)    \n",
    "\n",
    "# remove outliers from analysis (don't run this cell twice to avoid removing outliers after already excluding outliers)\n",
    "# remove_record(\"consumers\", consumers_screening[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='propensity-score-matching'></a>\n",
    "### G. Propensity Score Matching\n",
    "To reduce bias of distribution overlap and different density weighting, we rebalance our data through matching non-treated users to treated ones on similar covariate values. First, we estimate a probit model of receiving treatment on the number of followers, the number of followings, the adoption speed of Instagram users, and the percentage of image posts relative to all types of media posts. Second, we compute the Mahalanobis distance for each treated and control user pair and select unique matches sequentially, in order of closeness of their Mahalanobis distances. We match without replacement such that control units are only allowed to be used as a match once. Each treatment unit is matched with a single control unit as a higher number of matches deteriorates matching quality significantly. Third, we conduct an imbalance check before and after matching of which the results are reported in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# prepare data for propensity score matching (treatment/control country), follower/following count, image share, days since adoption\n",
    "consumers_query = \n",
    "\"\n",
    "SELECT w.username, treatment_control, CAST(followers_count AS NUMERIC), CAST(following_count AS NUMERIC), image, days_since_adoption \n",
    "FROM consumers_profile w \n",
    "INNER JOIN \n",
    "    (SELECT username, CASE     \n",
    "        WHEN country in ('australia', 'brazil', 'canada', 'italy') THEN 1   \n",
    "        ELSE 0 END as treatment_control \n",
    "    FROM consumers_country cc WHERE country != 'brazil') x ON w.username = x.username\n",
    "\n",
    "INNER JOIN (SELECT w.username, AVG(CAST(image_count AS DECIMAL) / CAST(posts_count AS DECIMAL)) as image \n",
    "            FROM consumers_profile w \n",
    "            INNER JOIN (SELECT username, SUM(CASE WHEN content_type = 'GraphImage' THEN 1 END) as image_count \n",
    "            FROM consumers_posts GROUP BY username) l ON l.username = w.username GROUP BY w.username) y ON y.username = w.username \n",
    "\n",
    "INNER JOIN (SELECT username, DATE_PART('day', '2020-06-01'::timestamp - MIN(timestamp)) as days_since_adoption \n",
    "            FROM consumers_posts GROUP BY username) z ON z.username = y.username\n",
    "\"\n",
    "\n",
    "PSM = function(df){  \n",
    "    # propensity score matching\n",
    "    Tr = cbind(as.vector(df$treatment_control))\n",
    "    X = as.matrix(df[,c('followers_count', 'following_count', 'days_since_adoption', 'image')])\n",
    "    \n",
    "    # replace NAs in the image column with zero\n",
    "    X[is.na(X)] = 0 \n",
    "    \n",
    "    glm1 = glm(Tr ~ X, family=binomial)\n",
    "    \n",
    "    rr1 = Match(Tr=Tr, X=glm1$fitted, replace = FALSE, Weight=1, M=1)\n",
    "    summary(rr1)\n",
    "  \n",
    "    # check balancing properties (results may deviate between bootstrap iterations)\n",
    "    MatchBalance(Tr ~ X, match.out = rr1, nboots=10000)\n",
    "  \n",
    "    # store indices of matched users\n",
    "    treatment = data.frame(df[rr1$index.treated,'username'], 'treatment')\n",
    "    colnames(treatment) = c(\"username\", \"type\")\n",
    "    control = data.frame(df[rr1$index.control,'username'], 'control')\n",
    "    colnames(control) = c(\"username\", \"type\")\n",
    "    return(rbind(treatment, control))\n",
    "}\n",
    "\n",
    "consumers_PSM_input = dbGetQuery(con, consumers_query)\n",
    "\n",
    "# lines below are commented to ensure consistency with paper (PSM results may slightly deviate for each run)\n",
    "# consumers_PSM = PSM(consumers_PSM_input)\n",
    "# dbWriteTable(con, \"consumers_psm\", consumers_PSM, overwrite = TRUE, row.names = FALSE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/instagram_header.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Klaasse Bos, R.J. (2020). Web Appendix: Goodbye Likes, Hello Mental Health: How Hiding Like Counts Affects User Behavior & Self-Esteem.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
